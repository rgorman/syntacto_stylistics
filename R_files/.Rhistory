sWord.table <- table(combined.content)
# normalize sWord.table by dividing by total sWords. Multiply by 100 to produce rate of sWord occurence per 100 sWords.
sWord.freq.table <- 100*(sWord.table/sum(sWord.table))
# insert sWord.freq.table into list
sWord.freq.table.list[[length(sWord.freq.table.list)+1]] <- sWord.freq.table
}
k <- 1
}
timestamp()
##------ Mon Nov 28 09:45:00 2016 ------##
#
single.token
rm(list = ls())
source("code/corpusFunctions.R")
input.dir <- "./working_input1"
files.v <- dir(path=input.dir, pattern=".*xml")
i <- 1
# create list object with no content. Vectors extracted from XML files will be stored here.
sWord.freq.table.list <- list()
timestamp()
##------ Mon Nov 28 09:53:55 2016 ------##
# set up loop to process each file in source directory
for (i in 1:length(files.v)) {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# extract all <word> elements and children into XmlNodeList object
word.nodes <- getNodeSet(doc.object, "//word")
# here we must split files into chunks
divisor <- length(word.nodes)/40
max.length <- length(word.nodes)/divisor
x <- seq_along(word.nodes)
chunks.l <- split(word.nodes, ceiling(x/max.length))
# set up a loop to process each chunk produced by preceding code in matrix
# set up list to store results of processsing
sWord.nodes.l <- list()
# extract sWord elements from each chunk
for (k in 1:length(chunks.l)) {
sWord.nodes.l[[k]] <- sapply(chunks.l[[k]], xmlChildren)
# create vector object to hold results
combined.content <- NULL
# set increment counter to 1
m <- 1
for (m in 1:length(sWord.nodes.l[[k]])) {
# extract sWord nodes for 1 token at a tiime; result is an XML list object with content which
# includes sWord xml tags.
single.token <- sWord.nodes.l[[k]][m]
# strip sWord level xml tags from data; result is a xml list object
content.nodes <- sapply(single.token[[1]], xmlChildren)
# create vector for output
extracted.content.v <- NULL
# set increment counter to 1
n <- 1
# iterate loop through each successive content.nodes object produced by matrix loop
for (n in 1:length(content.nodes)) {
# add successively extracted content to vector
extracted.content.v <- append(extracted.content.v, xmlValue(content.nodes[[n]]))
}
# collect output of nested loop in vector of all sWords for each token in chunk
combined.content <- append(combined.content, extracted.content.v)
}
# change sWord.contents vector to lower case
combined.content <- tolower(combined.content)
# create a contingency table of sWord.contents. The table lists nuber of occurences for all sWords.
sWord.table <- table(combined.content)
# normalize sWord.table by dividing by total sWords. Multiply by 100 to produce rate of sWord occurence per 100 sWords.
sWord.freq.table <- 100*(sWord.table/sum(sWord.table))
# insert sWord.freq.table into list
sWord.freq.table.list[[length(sWord.freq.table.list)+1]] <- sWord.freq.table
}
k <- 1
}
timestamp()
##------ Mon Nov 28 09:59:58 2016 ------##
# !! stop here for evaluation
single.token
rm(list = ls())
source("code/corpusFunctions.R")
input.dir <- "./working_input1"
files.v <- dir(path=input.dir, pattern=".*xml")
i <- 1
# create list object with no content. Vectors extracted from XML files will be stored here.
sWord.freq.table.list <- list()
timestamp()
##------ Mon Nov 28 10:02:56 2016 ------##
# set up loop to process each file in source directory
for (i in 1:length(files.v)) {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# extract all <word> elements and children into XmlNodeList object
word.nodes <- getNodeSet(doc.object, "//word")
# here we must split files into chunks
divisor <- length(word.nodes)/40
max.length <- length(word.nodes)/divisor
x <- seq_along(word.nodes)
chunks.l <- split(word.nodes, ceiling(x/max.length))
# set up a loop to process each chunk produced by preceding code in matrix
# set up list to store results of processsing
sWord.nodes.l <- list()
# extract sWord elements from each chunk
for (k in 1:length(chunks.l)) {
sWord.nodes.l[[k]] <- sapply(chunks.l[[k]], xmlChildren)
# create vector object to hold results
combined.content <- NULL
# set increment counter to 1
m <- 1
for (m in 1:length(sWord.nodes.l[[k]])) {
# extract sWord nodes for 1 token at a tiime; result is an XML list object with content which
# includes sWord xml tags.
single.token <- sWord.nodes.l[[k]][m]
# strip sWord level xml tags from data; result is a xml list object
content.nodes <- sapply(single.token[[1]], xmlChildren)
# create vector for output
extracted.content.v <- NULL
# set increment counter to 1
n <- 1
# iterate loop through each successive content.nodes object produced by matrix loop
for (n in 1:length(content.nodes)) {
# add successively extracted content to vector
extracted.content.v <- append(extracted.content.v, xmlValue(content.nodes[[n]]))
}
# collect output of nested loop in vector of all sWords for each token in chunk
combined.content <- append(combined.content, extracted.content.v)
}
# change sWord.contents vector to lower case
combined.content <- tolower(combined.content)
# create a contingency table of sWord.contents. The table lists nuber of occurences for all sWords.
sWord.table <- table(combined.content)
# normalize sWord.table by dividing by total sWords. Multiply by 100 to produce rate of sWord occurence per 100 sWords.
sWord.freq.table <- 100*(sWord.table/sum(sWord.table))
# insert sWord.freq.table into list
sWord.freq.table.list[[length(sWord.freq.table.list)+1]] <- sWord.freq.table
}
k <- 1
}
timestamp()
##------ Mon Nov 28 10:10:48 2016 ------##
length(sWord.freq.table.list)
summary(lengths(sWord.freq.table.list))
i <- 1
freqs.l <- list()
for (i in 1:length(sWord.freq.table.list)) {
freqs.l[[i]] <-data.frame(sWord.freq.table.list[[i]], ID=i,
stringsAsFactors=FALSE )
}
# convert to single matrix
timestamp()
##------ Mon Nov 28 10:20:25 2016 ------##
freqs.df <- do.call(rbind, freqs.l)
timestamp()
##------ Mon Nov 28 11:06:25 2016 ------##
#the result is a long form data frame
nrow(freqs.df)
assign("butt", 1:10)
object.size(freqs.df)
timestamp()
##------ Mon Nov 28 11:25:56 2016 ------##
# divide freqs.df into smaller data frames so that we can cross tabulate without running out of memory. Divide
# into at least 5 files
z <- ceiling(nrow(freqs.df)/10)
freqs.df[z,]
indexA <- which(freqs.df[,3] == freqs.df[z,3])
freqs.df1 <- freqs.df[1:indexA[length(indexA)],]
z <- 2 * (ceiling(nrow(freqs.df)/10))
freqs.df[z,]
indexB <- which(freqs.df[,3] == freqs.df[z,3])
freqs.df2 <- freqs.df[(indexA[length(indexA)]+1):indexB[length(indexB)],]
z <- 3 * (ceiling(nrow(freqs.df)/10))
freqs.df[z,]
indexC <- which(freqs.df[,3] == freqs.df[z,3])
freqs.df3 <- freqs.df[(indexB[length(indexB)]+1):indexC[length(indexC)],]
z <- 4 * (ceiling(nrow(freqs.df)/10))
freqs.df[z,]
indexD <- which(freqs.df[,3] == freqs.df[z,3])
freqs.df4 <- freqs.df[(indexC[length(indexC)]+1):indexD[length(indexD)],]
z <- 5 * (ceiling(nrow(freqs.df)/10))
freqs.df[z,]
indexE<- which(freqs.df[,3] == freqs.df[z,3])
freqs.df5 <- freqs.df[(indexD[length(indexD)]+1):indexE[length(indexE)],]
z <- 6 * (ceiling(nrow(freqs.df)/10))
freqs.df[z,]
indexF <- which(freqs.df[,3] == freqs.df[z,3])
freqs.df6 <- freqs.df[(indexE[length(indexE)]+1):indexF[length(indexF)],]
z <- 7 * (ceiling(nrow(freqs.df)/10))
freqs.df[z,]
indexG <- which(freqs.df[,3] == freqs.df[z,3])
freqs.df7 <- freqs.df[(indexF[length(indexF)]+1):indexG[length(indexG)],]
z <- 8 * (ceiling(nrow(freqs.df)/10))
freqs.df[z,]
indexH <- which(freqs.df[,3] == freqs.df[z,3])
freqs.df8 <- freqs.df[(indexG[length(indexG)]+1):indexH[length(indexH)],]
z <- 9 * (ceiling(nrow(freqs.df)/10))
freqs.df[z,]
indexI <- which(freqs.df[,3] == freqs.df[z,3])
freqs.df9 <- freqs.df[(indexH[length(indexH)]+1):indexI[length(indexI)],]
z <- 1 + indexI[length(indexI)]
freqs.df[z,]
freqs.df10 <- freqs.df[z:nrow(freqs.df),]
timestamp()
##------ Mon Nov 28 11:25:57 2016 ------##
# check sums for 10 files
sum(nrow(freqs.df1), nrow(freqs.df2), nrow(freqs.df3), nrow(freqs.df4), nrow(freqs.df5), nrow(freqs.df6),
nrow(freqs.df7), nrow(freqs.df8), nrow(freqs.df9), nrow(freqs.df10)) == nrow(freqs.df)
rm(freqs.df, chunks.l, doc.object, freqs.l, sWord.freq.table.list, sWord.nodes.l)
bookids.v <- gsub(".xml", "", files.v)
chunk.total <- NULL
i <- 1
for (i in 1:length(files.v))  {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# extract all <word> elements and children into XmlNodeList object
word.nodes <- getNodeSet(doc.object, "//word")
# here we must split files into chunks
divisor <- length(word.nodes)/50
max.length <- length(word.nodes)/divisor
x <- seq_along(word.nodes)
chunks.l <- split(word.nodes, ceiling(x/max.length))
chunk.total <- append(chunk.total, length(chunks.l))
}
# check to make sure final.df has correct number of rows
sum(chunk.total) == nrow(final.df)
sum(chunk.total)
chunk.total <- NULL
i <- 1
for (i in 1:length(files.v))  {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# extract all <word> elements and children into XmlNodeList object
word.nodes <- getNodeSet(doc.object, "//word")
# here we must split files into chunks
divisor <- length(word.nodes)/40
max.length <- length(word.nodes)/divisor
x <- seq_along(word.nodes)
chunks.l <- split(word.nodes, ceiling(x/max.length))
chunk.total <- append(chunk.total, length(chunks.l))
}
# check to make sure final.df has correct number of rows
sum(chunk.total) == nrow(final.df)
sum(chunk.total)
i <- 1
chunk.number <- NULL
ID.holder <- NULL
n <- 1
for (i in 1:length(chunk.total)) {
chunk.number <- seq_along(1:chunk.total[i])
# chunk.number corresponds to the different file names to used in row IDs
for (j in 1:length(chunk.number)) {
single.name <- paste(bookids.v[i], chunk.number[j], sep = "-", collapse = NULL)
ID.holder <- append(ID.holder, single.name)
n <-n+1
}
j <- 1
}
#
ID.matrix <- matrix(ID.holder, ncol = 1)
View(ID.matrix)
write.csv(ID.matrix, file = "Results_Nov-2016/Authors_AllGreekFiles_40tokens_Nov-28.csv")
rm(freqs.df, chunks.l, doc.object, freqs.l, sWord.freq.table.list, sWord.nodes.l)
# or run xtab by hand
timestamp()
##------ Mon Nov 28 11:45:56 2016 ------##
result1 <- xtabs(Freq ~ ID+combined.content, data=freqs.df1)
save(result1, file = "result1.R")
rm(result1)
result2 <- xtabs(Freq ~ ID+combined.content, data=freqs.df2)
save(result2, file = "result2.R")
rm(result2)
result3 <- xtabs(Freq ~ ID+combined.content, data=freqs.df3)
save(result3, file = "result3.R")
rm(result3)
result4 <- xtabs(Freq ~ ID+combined.content, data=freqs.df4)
save(result4, file = "result4.R")
rm(result4)
result5 <- xtabs(Freq ~ ID+combined.content, data=freqs.df5)
save(result5, file = "result5.R")
rm(result5)
result6 <- xtabs(Freq ~ ID+combined.content, data=freqs.df6)
save(result6, file = "result6.R")
rm(result6)
result7 <- xtabs(Freq ~ ID+combined.content, data=freqs.df7)
save(result7, file = "result7.R")
rm(result7)
result8 <- xtabs(Freq ~ ID+combined.content, data=freqs.df8)
save(result8, file = "result8.R")
rm(result8)
result9 <- xtabs(Freq ~ ID+combined.content, data=freqs.df9)
save(result9, file = "result9.R")
rm(result9)
result10 <- xtabs(Freq ~ ID+combined.content, data=freqs.df10)
save(result10, file = "result10.R")
rm(result10)
timestamp()
##------ Mon Nov 28 11:47:42 2016 ------##
save.image()
i <- 1
for(i in 1:6) { #-- Create objects  'r.1', 'r.2', ... 'r.6' --
nam <- paste("r", i, sep = ".")
assign(nam, 1:i)
}
nam <- paste("result", i, ".R", sep = "")
assign(nam, 1:100)
i <- 1
for (i in 1:10) {
nam <- paste("result", i, ".R", sep = "")
assign(nam, i:100)
}
freqs.dfB <- freqs.df1[1:1000,]
i <- 1
nam <- paste("result", i, ".R", sep = "")
assign(nam, xtabs(Freq ~ ID+combined.content, data=freqs.dfB))
nam <- paste("result", i, ".R", sep = "")
nam2 <- paste("freqs.df", i, sep = "")
nam2
assign(nam2, freqs.dfB)
assign(nam, xtabs(Freq ~ ID+combined.content, data=nam2))
nam <- paste("result", i, ".R", sep = "")
assign(nam2, freqs.dfB)
nam2 <- assign(nam2, freqs.dfB)
assign(nam, xtabs(Freq ~ ID+combined.content, data=nam2))
freqs.dfB1 <- freqs.df1[1:1000,]
nam2
nam2 <- paste("freqs.df", i, sep = "")
nam2
get(nam2)
rm(nam, nam2)
rm(result1.R)
i <- 1
nam <- paste("result", i, ".R", sep = "")
nam2 <- paste("freqs.df", i, sep = "")
nam2 <- assign(nam2, get(nam2))
assign(nam, xtabs(Freq ~ ID+combined.content, data=nam2))
rm(nam, nam2)
df1 <- as.data.frame(results1)
df1 <- as.data.frame(result1)
df1 <- as.data.frame(result1.R)
View(df1)
nam <- paste("result", i, sep = "")
nam2 <- paste("freqs.df", i, sep = "")
nam2 <- assign(nam2, get(nam2))
assign(nam, xtabs(Freq ~ ID+combined.content, data=nam2))
rm(nam, nam2)
dim(result1)
df1 <- as.data.frame(result1)
dim(df1)
freqs.df2
row.names(freqs.df2[1:10,])
as.data.frame.matrix(result1)
df1 <- as.data.frame.matrix(result1)
df1[, 1:100]
View(df1[, 1:10])
i <- 1
nam <- paste("result", i, sep = "")
nam2 <- paste("freqs.df", i, sep = "")
nam3 <- paste("result", i, ".R", sep = "")
nam2 <- assign(nam2, get(nam2))
assign(nam, xtabs(Freq ~ ID+combined.content, data=nam2))
save(nam, file = nam3)
rm(nam, nam2, nam3)
nam <- paste("result", i, sep = "")
rm(get(nam))
rm(paste("result", i, sep = ""))
rm(paste("result", i, sep = ""))
nam <- paste("result", i, sep = "")
rm(nam, nam2, nam3)
nam <- paste("result", i, sep = "")
nam
get(nam)
nam3 <- paste("result", i, ".R", sep = "")
nam3
rm(nam[1])
nam
as>character(nam[1])
as.character(nam[1])
result <- as.character(nam[1])
rm(result)
result <- as.character(nam[1])
rm(result[1])
ls()
rm(list = nam)
rm(list = nam2)
rm(list = nam3)
nam <- paste("result", i, sep = "")
nam2 <- paste("freqs.df", i, sep = "")
nam3 <- paste("result", i, ".R", sep = "")
nam2 <- assign(nam2, get(nam2))
assign(nam, xtabs(Freq ~ ID+combined.content, data=nam2))
catal <- c(nam, nam2, nam3)
rm(list = catal)
rm(list = nam, nam2, nam3)
load()
load(.Rdata)
load(.RData)
load(file =".RData")
nrow(freqs.df1)
nrow(freqs.df1)/10000
ceiling(nrow(freqs.df1)/10000)
i <- 1
nam <- paste("result", i, sep = "")
nam2 <- paste("freqs.df", i, sep = "")
nam3 <- paste("result", i, ".R", sep = "")
nam2 <- assign(nam2, get(nam2))
assign(nam, xtabs(Freq ~ ID+combined.content, data=nam2))
z <- ceiling(nrow(freqs.df)/50000)
z <- ceiling(nrow(freqs.df1)/50000)
freqs.df1[z,]
indexA <- which(freqs.df[,3] == freqs.df[z,3])
indexA <- which(freqs.df1[,3] == freqs.df1[z,3])
input.df <- freqs.df[1:indexA[length(indexA)],]
input.df <- freqs.df1[1:indexA[length(indexA)],]
i <- 1
nam <- paste("result", i, sep = "")
nam3 <- paste("result", i, ".R", sep = "")
assign(nam, xtabs(Freq ~ ID+combined.content, data=input.df))
save(nam, file = nam3)
rm(list = nam, nam2, nam3)
for (i in 1:z) {
indexA <- which(freqs.df1[,3] == freqs.df1[z,3])
input.df <- freqs.df1[1:indexA[length(indexA)],]
nam <- paste("result", i, sep = "")
nam3 <- paste("result", i, ".R", sep = "")
nam2 <- assign(nam2, get(nam2))
assign(nam, xtabs(Freq ~ ID+combined.content, data=input.df))
save(nam, file = nam3)
rm(list = nam, nam2, nam3)
}
for (i in 1:z) {
indexA <- which(freqs.df1[,3] == freqs.df1[z,3])
input.df <- freqs.df1[1:indexA[length(indexA)],]
nam <- paste("result", i, sep = "")
nam3 <- paste("result", i, ".R", sep = "")
assign(nam, xtabs(Freq ~ ID+combined.content, data=input.df))
save(nam, file = nam3)
rm(list = nam, nam2, nam3)
}
i <- 1
z <- ceiling(nrow(freqs.df1)/50000)
for (i in 1:z) {
indexA <- which(freqs.df1[,3] == freqs.df1[z,3])
input.df <- freqs.df1[1:indexA[length(indexA)],]
nam <- paste("result", i, sep = "")
nam3 <- paste("result", i, ".R", sep = "")
assign(nam, xtabs(Freq ~ ID+combined.content, data=input.df))
save(nam, file = nam3)
rm(list = nam, nam3)
}
rm(list = ls())
source("code/corpusFunctions.R")
input.dir <- "./working_input1"
files.v <- dir(path=input.dir, pattern=".*xml")
i <- 1
# create list object with no content. Vectors extracted from XML files will be stored here.
sWord.freq.table.list <- list()
timestamp()
##------ Wed Nov 30 10:36:27 2016 ------##
# set up loop to process each file in source directory
for (i in 1:length(files.v)) {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# extract all <word> elements and children into XmlNodeList object
word.nodes <- getNodeSet(doc.object, "//word")
# here we must split files into chunks
divisor <- length(word.nodes)/40
max.length <- length(word.nodes)/divisor
x <- seq_along(word.nodes)
chunks.l <- split(word.nodes, ceiling(x/max.length))
# set up a loop to process each chunk produced by preceding code in matrix
# set up list to store results of processsing
sWord.nodes.l <- list()
# extract sWord elements from each chunk
for (k in 1:length(chunks.l)) {
sWord.nodes.l[[k]] <- sapply(chunks.l[[k]], xmlChildren)
# create vector object to hold results
combined.content <- NULL
# set increment counter to 1
m <- 1
for (m in 1:length(sWord.nodes.l[[k]])) {
# extract sWord nodes for 1 token at a tiime; result is an XML list object with content which
# includes sWord xml tags.
single.token <- sWord.nodes.l[[k]][m]
# strip sWord level xml tags from data; result is a xml list object
content.nodes <- sapply(single.token[[1]], xmlChildren)
# create vector for output
extracted.content.v <- NULL
# set increment counter to 1
n <- 1
# iterate loop through each successive content.nodes object produced by matrix loop
for (n in 1:length(content.nodes)) {
# add successively extracted content to vector
extracted.content.v <- append(extracted.content.v, xmlValue(content.nodes[[n]]))
}
# collect output of nested loop in vector of all sWords for each token in chunk
combined.content <- append(combined.content, extracted.content.v)
}
# change sWord.contents vector to lower case
combined.content <- tolower(combined.content)
# create a contingency table of sWord.contents. The table lists nuber of occurences for all sWords.
sWord.table <- table(combined.content)
# normalize sWord.table by dividing by total sWords. Multiply by 100 to produce rate of sWord occurence per 100 sWords.
sWord.freq.table <- 100*(sWord.table/sum(sWord.table))
# insert sWord.freq.table into list
sWord.freq.table.list[[length(sWord.freq.table.list)+1]] <- sWord.freq.table
}
k <- 1
}
timestamp()
##------ Wed Nov 30 10:50:17 2016 ------##
i <- 1
freqs.l <- list()
for (i in 1:length(sWord.freq.table.list)) {
freqs.l[[i]] <-data.frame(sWord.freq.table.list[[i]], ID=i,
stringsAsFactors=FALSE )
}
save.image("~/syntacto_stylistics/R_files/test-11-30.RData")
