2*2
install.packages(c("XML", "textreuse"))
p <- 29.46
i <- 24.57
r <- 250
r - (p+1)
250-54.03
195.97 + 25.23
62*31
172/15
30+420
372+72
315+121
172+220+50
a <- 15
b <- 16
c <- 15
d <- 15
a * 12
a * 15
b * 20
b * 8
c * 25
c * 5
d * 2
d * 28
16*15
15*17
15*19
seq(1:150)
f <- seq(1:150)
length(f)
b * 20
mast <- 15*17
march <- 15*2
seq(301, 546)
length(seq(301, 546))
a * 12
a <- 15
b <- 16
c <- 15
d <- 15
a * 12
card <-  seq(1101, 1172)
card <-  seq(1001, 1172)
length(card)
card <-  seq(2001, 2219)
length(card)
54.03-250
195.97+31.43
7/16
14/16
12/16
10/16
5/16
9/16
19/23
20/22
19/21
12/19
15/19
20/21
18/21
17/19
18/22
21/23
18/22
15/17
17/21
18/22
14/15
11/14
12/14
13/15
10/14
20/23
11/14
12/15
14/15
12/14
11/14
11/14
17/23
10/14
19*60
17*60
60*9
1.309 * 200
3*1.5
p <- 32.1
i <- 21.93
p + i
250- (p+i)
195.97 + p
setwd("~/syntacto_stylistics/R_files")
library(XML)
source("code/corpusFunctions.R")
input.dir <- "../sWord_levels/files_with_sWords/VG_files"
files.v <- dir(path=input.dir, pattern=".*xml")
i <- 1
sWord.freq.table.list <- list()
# set up loop to process each file in source directory
for (i in 1:length(files.v)) {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# extract all <word> elements and children into XmlNodeList object
word.nodes <- getNodeSet(doc.object, "//word")
# here we must split files into chunks
divisor <- length(word.nodes)/1000
max.length <- length(word.nodes)/divisor
x <- seq_along(word.nodes)
chunks.l <- split(word.nodes, ceiling(x/max.length))
# set up a loop to process each chunk produced by preceding code in matrix
# set up list to store results of processsing
sWord.nodes.l <- list()
# extract sWord elements from each chunk
for (k in 1:length(chunks.l)) {
sWord.nodes.l[[k]] <- sapply(chunks.l[[k]], xmlChildren)
# create vector object to hold results
combined.content <- NULL
# set increment counter to 1
m <- 1
for (m in 1:length(sWord.nodes.l[[k]])) {
# extract sWord nodes for 1 token at a tiime; result is an XML list object with content which
# includes sWord xml tags.
single.token <- sWord.nodes.l[[k]][m]
# strip sWord level xml tags from data; result is a xml list object
content.nodes <- sapply(single.token[[1]], xmlChildren)
# create vector for output
extracted.content.v <- NULL
# set increment counter to 1
n <- 1
# iterate loop through each successive content.nodes object produced by matrix loop
for (n in 1:length(content.nodes)) {
# add successively extracted content to vector
extracted.content.v <- append(extracted.content.v, xmlValue(content.nodes[[n]]))
}
# collect output of nested loop in vector of all sWords for each token in chunk
combined.content <- append(combined.content, extracted.content.v)
}
# change sWord.contents vector to lower case
combined.content <- tolower(combined.content)
# create a contingency table of sWord.contents. The table lists nuber of occurences for all sWords.
sWord.table <- table(combined.content)
# normalize sWord.table by dividing by total sWords. Multiply by 100 to produce rate of sWord occurence per 100 sWords.
sWord.freq.table <- 100*(sWord.table/sum(sWord.table))
# insert sWord.freq.table into list
sWord.freq.table.list[[length(sWord.freq.table.list)+1]] <- sWord.freq.table
}
k <- 1
}
i <- 1
freqs.l <- list()
for (i in 1:length(sWord.freq.table.list)) {
freqs.l[[i]] <-data.frame(sWord.freq.table.list[[i]], ID=seq_along(sWord.freq.table.list[[i]]),
stringsAsFactors=FALSE )
}
freqs.df <- do.call(rbind, freqs.l)
View(freqs.df)
dim(freqs.df)
bookids.v <- gsub(".xml", "", files.v)
chunk.total <- NULL
i <- 1
for (i in 1:length(files.v))  {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# extract all <word> elements and children into XmlNodeList object
word.nodes <- getNodeSet(doc.object, "//word")
# here we must split files into chunks
divisor <- length(word.nodes)/1000
max.length <- length(word.nodes)/divisor
x <- seq_along(word.nodes)
chunks.l <- split(word.nodes, ceiling(x/max.length))
chunk.total <- append(chunk.total, length(chunks.l))
}
i <- 1
chunk.number <- NULL
ID.holder <- NULL
n <- 1
for (i in 1:length(chunk.total)) {
chunk.number <- seq_along(1:chunk.total[i])
# chunk.number corresponds to the different file names to used in row IDs
for (j in 1:length(chunk.number)) {
single.name <- paste(bookids.v[i], chunk.number[j], sep = "-", collapse = NULL)
ID.holder <- append(ID.holder, rep(single.name, nrow(freqs.l[[n]])))
n <-n+1
}
j <- 1
}
ID.holder[100000]
freqs.df$ID <- ID.holder
result.t <- xtabs(Freq ~ ID+combined.content, data=freqs.df)
dim(result.t)
final.df <- as.data.frame.matrix(result.t)
rm(result.t)
freq.means.v <- colMeans(final.df[, ])
keepers.v <- which(freq.means.v >=.009)
smaller.df <- final.df[, keepers.v]
dim(smaller.df)
library(e1071)
library (gmodels)
library(klaR)
chunk.authors.m <- read.csv(file = "Rresults/matrices/AuthorFactor_1000tokenChunks_June-13-2016.csv", stringsAsFactors = FALSE)
author.v <- chunk.authors.m[, 2]
probs.m <- read.csv(file = "Rresults/matrices/chunk_ratios_1000.csv", stringsAsFactors = FALSE)
testing.index <- sample(1:175, 18, prob = probs.m[, 2])
testing.data <- smaller.df[testing.index, ]
training.data <- smaller.df[-testing.index, ]
testing.classes <- as.factor(author.v[testing.index])
training.classes <- as.factor(author.v[-testing.index])
svm.model <- svm(training.data, training.classes, kernel = "linear", scale = FALSE)
svm.results <- <- predict(svm.model, testing.data)
svm.results <- predict(svm.model, testing.data)
svm.results
length(svm.results)
svm.results[[4]]
plot.svm(svm.model, training.data)
library(e1071)
plot.svm(svm.model, training.data)
library("e1071", lib.loc="~/R/win-library/3.2")
plot.svm(svm.model, training.data)
summary(svm.model)
svm.model$SV
svm.model$cost
svm.model$index
summary(svm.model)
svm.model$type
attr(svm.model)
attr(svm.model, "type")
attr(svm.model, "cost")
svm.model <- svm(training.data, training.classes, kernel = "linear", scale = FALSE, cost = 1000)
svm.results <- predict(svm.model, testing.data)
svm.results
i <- 1
svm.results.l <- list()
svm.error.matrix.l <- list()
testing.classes.l <- list()
i <- 1
for (i in 1:100) {
testing.index <- sample(1:175, 18, prob = probs.m[, 2])
testing.data <- smaller.df[testing.index, ]
training.data <- smaller.df[-testing.index, ]
testing.classes <- as.factor(author.v[testing.index])
training.classes <- as.factor(author.v[-testing.index])
svm.model <- svm(training.data, training.classes, kernel = "linear", scale = FALSE, cost = 1000)
svm.results.l[[i]] <- predict(svm.model, testing.data)
svm.error.matrix.l[[i]] <- errormatrix(testing.classes, svm.results.l[[i]])
}
a <- do.call(rbind, svm.error.matrix.l)
View(a)
sum(a[, 6])
(1800- (sum(a[, 6])/2))/1800
svm.results.l <- list()
svm.error.matrix.l <- list()
testing.classes.l <- list()
i <- 1
for (i in 1:100) {
testing.index <- sample(1:175, 18, prob = probs.m[, 2])
testing.data <- smaller.df[testing.index, ]
training.data <- smaller.df[-testing.index, ]
testing.classes <- as.factor(author.v[testing.index])
training.classes <- as.factor(author.v[-testing.index])
svm.model <- svm(training.data, training.classes, kernel = "linear", scale = FALSE, cost = 100)
svm.results.l[[i]] <- predict(svm.model, testing.data)
svm.error.matrix.l[[i]] <- errormatrix(testing.classes, svm.results.l[[i]])
}
a <- do.call(rbind, svm.error.matrix.l)
sum(a[, 6])
(1800- (sum(a[, 6])/2))/1800
svm.results.l <- list()
svm.error.matrix.l <- list()
testing.classes.l <- list()
i <- 1
for (i in 1:100) {
testing.index <- sample(1:175, 18, prob = probs.m[, 2])
testing.data <- smaller.df[testing.index, ]
training.data <- smaller.df[-testing.index, ]
testing.classes <- as.factor(author.v[testing.index])
training.classes <- as.factor(author.v[-testing.index])
svm.model <- svm(training.data, training.classes, kernel = "linear", scale = FALSE, cost = 10)
svm.results.l[[i]] <- predict(svm.model, testing.data)
svm.error.matrix.l[[i]] <- errormatrix(testing.classes, svm.results.l[[i]])
}
a <- do.call(rbind, svm.error.matrix.l)
sum(a[, 6])
(1800- (sum(a[, 6])/2))/1800
obj <- tune.svm(training.data, training.classes, kernel = "linear", scale = FALSE, cost = seq(1, 1000, by = 5))
obj$best.parameters
obj$best.performance
obj$method
obj$performances
obj$best.model
svm.results <- predict(svm.model, testing.data)
svm.results
svm.model <- svm(training.data, training.classes, kernel = "linear", scale = FALSE, cost = 0.5)
svm.results.l[[i]] <- predict(svm.model, testing.data)
svm.results.l <- list()
svm.error.matrix.l <- list()
testing.classes.l <- list()
i <- 1
for (i in 1:100) {
testing.index <- sample(1:175, 18, prob = probs.m[, 2])
testing.data <- smaller.df[testing.index, ]
training.data <- smaller.df[-testing.index, ]
testing.classes <- as.factor(author.v[testing.index])
training.classes <- as.factor(author.v[-testing.index])
svm.model <- svm(training.data, training.classes, kernel = "linear", scale = FALSE, cost = 0.5)
svm.results.l[[i]] <- predict(svm.model, testing.data)
svm.error.matrix.l[[i]] <- errormatrix(testing.classes, svm.results.l[[i]])
}
a <- do.call(rbind, svm.error.matrix.l)
sum(a[, 6])
(1800- (sum(a[, 6])/2))/1800
svm.results.l <- list()
svm.error.matrix.l <- list()
testing.classes.l <- list()
i <- 1
for (i in 1:100) {
testing.index <- sample(1:175, 18, prob = probs.m[, 2])
testing.data <- smaller.df[testing.index, ]
training.data <- smaller.df[-testing.index, ]
testing.classes <- as.factor(author.v[testing.index])
training.classes <- as.factor(author.v[-testing.index])
svm.model <- svm(training.data, training.classes, kernel = "linear", scale = FALSE, cost = 0.1)
svm.results.l[[i]] <- predict(svm.model, testing.data)
svm.error.matrix.l[[i]] <- errormatrix(testing.classes, svm.results.l[[i]])
}
a <- do.call(rbind, svm.error.matrix.l)
sum(a[, 6])
(1800- (sum(a[, 6])/2))/1800
t
t
svm.results.l <- list()
svm.error.matrix.l <- list()
testing.classes.l <- list()
i <- 1
for (i in 1:100) {
testing.index <- sample(1:175, 18, prob = probs.m[, 2])
testing.data <- smaller.df[testing.index, ]
training.data <- smaller.df[-testing.index, ]
testing.classes <- as.factor(author.v[testing.index])
training.classes <- as.factor(author.v[-testing.index])
svm.model <- svm(training.data, training.classes, kernel = "linear", scale = FALSE, cost = 2)
svm.results.l[[i]] <- predict(svm.model, testing.data)
svm.error.matrix.l[[i]] <- errormatrix(testing.classes, svm.results.l[[i]])
}
a <- do.call(rbind, svm.error.matrix.l)
sum(a[, 6])
(1800- (sum(a[, 6])/2))/1800
rm(list = ls())
input.dir <- "../sWord_levels/files_with_sWords/VG_files"
files.v <- dir(path=input.dir, pattern=".*xml")
freq.table.l <- list()
i <- 1
for (i in 1: length(files.v)) {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# input all geodesics (i.e., full sWords from leaf to root) into object "geodesics".
geodesics <- getNodeSet(doc.object, "//word/*[last()]")
# extract content of geodesic  elements and put it into a character vector object
geodesic.content <- paste(sapply(geodesics, xmlValue), collapse=NULL)
# convert geodesic.content to lower case
geodesic.content <- tolower(geodesic.content)
# here we  split object "words" into chunks;
divisor <- length(geodesics)/1000 # this divisor indicates size of word chunks; the result it the real number representing
# the number of x-length chunks in "words"
# a wrapper for use in ceiling(); it is the same as the integer supplied above.
max.length <- length(geodesics)/divisor
# create a vector of integers corresponding to each <word> element in "words"
x <- seq_along(geodesics)
# create a list object of elements each with no more than selected chunk size of elements (the last set in list will be smaller
# unless the number of words in file is evenly divisible). The function ceililng() is used to ensure an integer for the argument of
# the function split().
chunks.l <- split(geodesic.content, ceiling(x/max.length))
# create contingincy table; this device records the number of occurence of each type of sWord in each chunk
geodesic.table <- lapply(chunks.l, table)
# create relative frequency table
freq.table <- lapply(geodesic.table, prop.table)
# store output of loop
freq.table.l[[i]] <- freq.table
}
#this function is applied to a list of lists of tables to convert to matrix. function must be run to be accessed later.
my.apply <- function(x){
my.list <-mapply(data.frame, ID=seq_along(x),
x, SIMPLIFY=FALSE,
MoreArgs=list(stringsAsFactors=FALSE))
my.df <- do.call(rbind, my.list)
return(my.df)
}
freqs.l <- list()
freqs.l <- lapply(freq.table.l, my.apply)
freqs.df <- do.call(rbind, freqs.l)
bookids.v <- gsub(".xml", "", files.v)
chunk.total <- NULL
i <- 1
for (i in 1:length(files.v))  {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# extract all <word> elements and children into XmlNodeList object
word.nodes <- getNodeSet(doc.object, "//word")
# here we must split files into chunks
divisor <- length(word.nodes)/1000
max.length <- length(word.nodes)/divisor
x <- seq_along(word.nodes)
chunks.l <- split(word.nodes, ceiling(x/max.length))
chunk.total <- append(chunk.total, length(chunks.l))
}
ID.holder <- NULL
i <- 1
single.name <- NULL
for (i in 1:length(bookids.v)) {
for (j in 1:length(freqs.l[[i]]$ID)) {
single.name <- paste(bookids.v[i], freqs.l[[i]]$ID[j], sep = "-", collapse = NULL)
ID.holder <- append(ID.holder, single.name)
}
z <-  2*i
j <- 1
}
freqs.df$ID <- ID.holder
result.t <- xtabs(Freq ~ ID+Var1, data=freqs.df)
dim(result.t)
final.df <- as.data.frame.matrix(result.t)
rm(result.t)
freq.means.v <- colMeans(final.df[, ])
keepers.v <- which(freq.means.v >=.00008)
smaller.df <- final.df[, keepers.v]
dim(smaller.df)
chunk.authors.m <- read.csv(file = "Rresults/matrices/AuthorFactor_1000tokenChunks_June-13-2016.csv", stringsAsFactors = FALSE)
author.v <- chunk.authors.m[, 2]
probs.m <- read.csv(file = "Rresults/matrices/chunk_ratios_1000.csv", stringsAsFactors = FALSE)
testing.index <- sample(1:175, 18, prob = probs.m[, 2])
testing.data <- smaller.df[testing.index, ]
training.data <- smaller.df[-testing.index, ]
testing.classes <- as.factor(author.v[testing.index])
training.classes <- as.factor(author.v[-testing.index])
svm.model <- svm(training.data, training.classes, kernel = "linear", scale = FALSE, cost = 1)
svm.results.l[[i]] <- predict(svm.model, testing.data)
svm.model <- svm(training.data, training.classes, kernel = "linear", scale = TRUE, cost = 1)
svm.results <- predict(svm.model, testing.data)
svm.results
obj <- tune.svm(training.data, training.classes, kernel = "linear", scale = TRUE, cost = seq(1, 1000, by = 5))
obj$best.model
obj$best.performance
obj$best.parameters
