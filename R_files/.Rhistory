2*2
install.packages(c("XML", "textreuse"))
p <- 29.46
i <- 24.57
r <- 250
r - (p+1)
250-54.03
195.97 + 25.23
setwd("~/syntacto_stylistics/R_files")
rm(list = ls())
library(XML)
input.dir <- "../sWord_levels/files_with_sWords/VG_files"
files.v <- dir(path=input.dir, pattern=".*xml")
sWord.freq.table.list <- list()
for (i in 1:length(files.v)) {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# extract all <word> elements and children into XmlNodeList object
word.nodes <- getNodeSet(doc.object, "//word")
# here we must split files into chunks
divisor <- length(word.nodes)/100
max.length <- length(word.nodes)/divisor
x <- seq_along(word.nodes)
chunks.l <- split(word.nodes, ceiling(x/max.length))
# set up a loop to process each chunk produced by preceding code in matrix loop
# set up list to store results of processsing
sWord.nodes.l <- list()
# extract sWord elements from each chunk
for (k in 1:length(chunks.l)) {
sWord.nodes.l[[k]] <- sapply(chunks.l[[k]], xmlChildren)
# create vector object to hold results
combined.content <- NULL
# set increment counter to 1
m <- 1
for (m in 1:length(sWord.nodes.l[[k]])) {
# extract sWord nodes for 1 token at a tiime; result is an XML list object with content which
# includes sWord xml tags.
single.token <- sWord.nodes.l[[k]][m]
# strip sWord level xml tags from data; result is a xml list object
content.nodes <- sapply(single.token[[1]], xmlChildren)
# create vector for output
extracted.content.v <- NULL
# set increment counter to 1
n <- 1
# iterate loop through each successive content.nodes object produced by matrix loop
for (n in 1:length(content.nodes)) {
# add successively extracted content to vector
extracted.content.v <- append(extracted.content.v, xmlValue(content.nodes[[n]]))
}
# collect output of nested loop in vector of all sWords for each token in chunk
combined.content <- append(combined.content, extracted.content.v)
}
# change sWord.contents vector to lower case
combined.content <- tolower(combined.content)
# create a contingency table of sWord.contents. The table lists nuber of occurences for all sWords.
sWord.table <- table(combined.content)
# normalize sWord.table by dividing by total sWords. Multiply by 100 to produce rate of sWord occurence per 100 sWords.
sWord.freq.table <- 100*(sWord.table/sum(sWord.table))
# insert sWord.freq.table into list
sWord.freq.table.list[[length(sWord.freq.table.list)+1]] <- sWord.freq.table
}
k <- 1
}
length(sWord.freq.table.list)
lengths(sWord.freq.table.list)
mean(lengths(sWord.freq.table.list))
summary(lengths(sWord.freq.table.list))
i <- 1
freqs.l <- list()
for (i in 1:length(sWord.freq.table.list)) {
freqs.l[[i]] <-data.frame(sWord.freq.table.list[[i]], ID=seq_along(sWord.freq.table.list[[i]]),
stringsAsFactors=FALSE )
}
freqs.df <- do.call(rbind, freqs.l)
bookids.v <- gsub(".xml", "", files.v)
chunk.total <- NULL
i <- 1
for (i in 1:length(files.v))  {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# extract all <word> elements and children into XmlNodeList object
word.nodes <- getNodeSet(doc.object, "//word")
# here we must split files into chunks
divisor <- length(word.nodes)/100
max.length <- length(word.nodes)/divisor
x <- seq_along(word.nodes)
chunks.l <- split(word.nodes, ceiling(x/max.length))
chunk.total <- append(chunk.total, length(chunks.l))
}
i <- 1
chunk.number <- NULL
ID.holder <- NULL
n <- 1
for (i in 1:length(chunk.total)) {
chunk.number <- seq_along(1:chunk.total[i])
# chunk.number corresponds to the different file names to used in row IDs
for (j in 1:length(chunk.number)) {
single.name <- paste(bookids.v[i], chunk.number[j], sep = "-", collapse = NULL)
ID.holder <- append(ID.holder, rep(single.name, nrow(freqs.l[[n]])))
n <-n+1
}
j <- 1
}
ID.holder[384:387]
freqs.df2 <- freqs.df
freqs.df2$ID <- ID.holder
rm(freqs.df)
result.t <- xtabs(Freq ~ ID+combined.content, data=freqs.df2)
dim(result.t)
freq.means.v <- colMeans(final.df[, ])
freq.means.v <- colMeans(final.df2[, ])
final.df <- as.data.frame.matrix(result.t)
freq.means.v <- colMeans(final.df[, ])
keepers.v <- which(freq.means.v >=.003)
freq.means.v <- colMeans(final.df[, ])
help("memory.size")
memory.size()
re(result.t)
rm(result.t)
freq.means.v <- colMeans(final.df[, ])
keepers.v <- which(freq.means.v >=.003)
smaller.df <- final.df[, keepers.v]
dim(smaller.df)
ordered.df <- smaller.df[, order(colMeans(smaller.df), decreasing=TRUE)]
dim(smaller.df)
write.csv(ordered.df, file = "Rresults/matrices/sWordLevels_100tokenChunks_Feb-7-2016.csv")
rm(c(final.df, freqs.df2, smaller.df))
rm(final.df, freqs.df2, smaller.df)
smaller.df <- ordered.df
re(ordered.df)
rm(ordered.df)
rm(chunks.l, freq.means.v, freqs.l, ID.holder, sWord.freq.table.list, sWord.nodes.l, word.nodes)
chunk.ratios.m <- read.csv(file="Rresults/matrices/chunk_ratios_100.csv", stringsAsFactors = FALSE)
chunk.ratios.m[,2]
author.factor <- NULL
author.factor <- append(author.factor, rep("Diodorus", 243))
author.factor <- append(author.factor, rep("Herodotus", 312))
author.factor <- append(author.factor, rep("Plutarch", 207))
author.factor <- append(author.factor, rep("Polybius", 721))
author.factor <- append(author.factor, rep("Thucydides", 240))
svm.results.l <- list()
svm.error.matrix.l <- list()
testing.classes.l <- list()
i <- 1
testing.index.v <- sample (seq (1, nrow(smaller.df)), 170, prob = chunk.ratios.m[, 2])
testing.index.v
for (i in 1:1000) {
#create vector of random integers = 10% of obs in smaller.df
testing.index.v <- sample (seq (1, nrow(smaller.df)), 170, prob = chunk.ratios.m[, 2])
#create training and testing data matrices using testing.index.v and its inverse
testing.data <- smaller.df[testing.index.v, ]
training.data <- smaller.df[-testing.index.v, ]
#create vectors of factors giving classes (here = authors) of each row in testing.data and training.data
training.classes <- as.factor(author.factor[-testing.index.v])
testing.classes <- as.factor(author.factor[testing.index.v])
model.svm <- svm(training.data, training.classes, kernel = "linear", scale = FALSE)
svm.results.l[[i]] <- predict(model.svm, testing.data)
svm.error.matrix.l[[i]] <- errormatrix(testing.classes, svm.results.l[[i]])
}
require(e1071)
require(gmodels)
require(klaR)
svm.results.l <- list()
svm.error.matrix.l <- list()
testing.classes.l <- list()
i <- 1
for (i in 1:1000) {
#create vector of random integers = 10% of obs in smaller.df
testing.index.v <- sample (seq (1, nrow(smaller.df)), 170, prob = chunk.ratios.m[, 2])
#create training and testing data matrices using testing.index.v and its inverse
testing.data <- smaller.df[testing.index.v, ]
training.data <- smaller.df[-testing.index.v, ]
#create vectors of factors giving classes (here = authors) of each row in testing.data and training.data
training.classes <- as.factor(author.factor[-testing.index.v])
testing.classes <- as.factor(author.factor[testing.index.v])
model.svm <- svm(training.data, training.classes, kernel = "linear", scale = FALSE)
svm.results.l[[i]] <- predict(model.svm, testing.data)
svm.error.matrix.l[[i]] <- errormatrix(testing.classes, svm.results.l[[i]])
}
a <- do.call(rbind, svm.error.matrix.l)
View(a)
sum(a[,6])/2
sum(a[,6])
sum(a[,6])/2
(170000-(sum(a[,6])/2))/170000
write.csv(a, file="Rresults/svmError_matrix_100_Feb-7-2016.csv")
plot(model.svm)
plot(model.svm, testing.data)
