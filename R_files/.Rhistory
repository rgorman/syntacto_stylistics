freqs.l <- mapply(data.frame, ID=seq_along(book.freqs.l),
book.freqs.l, SIMPLIFY=FALSE, MoreArgs=list(stringsAsFactors=FALSE))
freqs.df <- do.call(rbind, freqs.l)
names(freqs.df)
result <- xtabs(Freq ~ ID+sword.content, data=freqs.df)
final.m <- apply(result, 2, as.numeric)
dim(final.m)
names_for_files.v <- gsub (".xml", "", files.v)
rownames(final.m) <-names_for_files.v
rownames(final.m)
smaller.m <- final.m[, apply(final.m,2,mean)>=.0025]
dim(smaller.m)
dist.smaller.m2 <- dist(scale(smaller.m), upper = FALSE, diag = TRUE)
dist.smaller.m2
groups2 <- hclust(dist.smaller.m2, method="ward.D2")
plot(groups2, hang=-1, xlab="Perseus Treebank", main="Dendrogram of Ancient Languages Dependency Treebank")
dend2 <- as.dendrogram(groups2)
plot(dend2)
labels_colors(dend2)[1] <- "red" # Diodorus
labels_colors(dend2)[c(2, 3)] <- "indianred1" # Plb
labels_colors(dend2)[4] <- "hotpink1" # # Lysias
labels_colors(dend2)[5] <- "darksalmon" # Thuc.
labels_colors(dend2)[6] <- "darkred" # Athen.
labels_colors(dend2)[7] <- "darkorange" # Hdt
labels_colors(dend2)[c(8,9)] <- "deeppink" # Plutarch
labels_colors(dend2)[c( 11, 12, 13, 14, 15)] <- "forestgreen" # Sophocles
labels_colors(dend2)[c(10, 16, 17, 18, 19, 20, 21)] <- "darkgoldenrod" # Aeschylus
labels_colors(dend2)[c(22, 23, 24, 25, 26, 27)] <- "cornflowerblue" # Iliad
labels_colors(dend2)[c(28, 29, 30, 31, 32, 33)] <- "blue" # Odyssey
labels_colors(dend2)[c(34, 35, 36)] <- "blueviolet" # Hesiod
plot(dend2)
labels_colors(dend2)
par(dend2)
plot(dend2, main="Dendrogram of Ancient Languages Dependency Treebank")
plot(dend2, main="Dendrogram of Ancient Languages Dependency Treebank", sub = "poop")
plot(dend2, main="Dendrogram of Ancient Languages Dependency Treebank", sub = "this is the story of a man")
plot(dend2, main="Dendrogram of Ancient Languages Dependency Treebank", sub = "           this is the story of a man")
plot(dend2, main="Dendrogram of Ancient Languages Dependency Treebank", sub = "                        this is the story of a man")
plot(dend2, cex = 0.75, main="Dendrogram of Ancient Languages Dependency Treebank", sub = "this is the story of a man")
plot(dend2, cex = 0.5, main="Dendrogram of Ancient Languages Dependency Treebank", sub = "this is the story of a man")
plot(dend2, cex = 0.3, main="Dendrogram of Ancient Languages Dependency Treebank", sub = "this is the story of a man")
rm(list = ls())
source("code/corpusFunctions.R")
input.dir <- "../sWord_levels/VG_files_POS_and_REL_and_relpos"
files.v <- dir(path=input.dir, pattern=".*xml")
i <- 1
sWord.freq.table.list <- list()
# set up loop to process each file in source directory
for (i in 1:length(files.v)) {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# extract all <word> elements and children into XmlNodeList object
word.nodes <- getNodeSet(doc.object, "//word")
word.nodes <- word.nodes[sample(1:length(word.nodes), floor(length(word.nodes)/3))]
# here we must split files into chunks
divisor <- length(word.nodes)/40
max.length <- length(word.nodes)/divisor
x <- seq_along(word.nodes)
chunks.l <- split(word.nodes, ceiling(x/max.length))
# set up a loop to process each chunk produced by preceding code in matrix
# set up list to store results of processsing
sWord.nodes.l <- list()
# extract sWord elements from each chunk
for (k in 1:length(chunks.l)) {
sWord.nodes.l[[k]] <- sapply(chunks.l[[k]], xmlChildren)
# create vector object to hold results
combined.content <- NULL
# set increment counter to 1
m <- 1
for (m in 1:length(sWord.nodes.l[[k]])) {
# extract sWord nodes for 1 token at a tiime; result is an XML list object with content which
# includes sWord xml tags.
single.token <- sWord.nodes.l[[k]][m]
# strip sWord level xml tags from data; result is a xml list object
content.nodes <- sapply(single.token[[1]], xmlChildren)
# create vector for output
extracted.content.v <- NULL
# set increment counter to 1
n <- 1
# iterate loop through each successive content.nodes object produced by matrix loop
for (n in 1:length(content.nodes)) {
# add successively extracted content to vector
extracted.content.v <- append(extracted.content.v, xmlValue(content.nodes[[n]]))
}
# collect output of nested loop in vector of all sWords for each token in chunk
combined.content <- append(combined.content, extracted.content.v)
}
# change sWord.contents vector to lower case
combined.content <- tolower(combined.content)
# create a contingency table of sWord.contents. The table lists nuber of occurences for all sWords.
sWord.table <- table(combined.content)
# normalize sWord.table by dividing by total sWords. Multiply by 100 to produce rate of sWord occurence per 100 sWords.
sWord.freq.table <- 100*(sWord.table/sum(sWord.table))
# insert sWord.freq.table into list
sWord.freq.table.list[[length(sWord.freq.table.list)+1]] <- sWord.freq.table
}
k <- 1
}
length(sWord.freq.table.list)
lengths(sWord.freq.table.list)
mean(lengths(sWord.freq.table.list))
summary(lengths(sWord.freq.table.list))
i <- 1
freqs.l <- list()
for (i in 1:length(sWord.freq.table.list)) {
freqs.l[[i]] <-data.frame(sWord.freq.table.list[[i]], ID=seq_along(sWord.freq.table.list[[i]]),
stringsAsFactors=FALSE )
}
freqs.df <- do.call(rbind, freqs.l)
bookids.v <- gsub(".xml", "", files.v)
chunk.total <- NULL
i <- 1
for (i in 1:length(files.v))  {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# extract all <word> elements and children into XmlNodeList object
word.nodes <- getNodeSet(doc.object, "//word")
word.nodes <- word.nodes[sample(1:length(word.nodes), floor(length(word.nodes)/3))]
# here we must split files into chunks
divisor <- length(word.nodes)/40
max.length <- length(word.nodes)/divisor
x <- seq_along(word.nodes)
chunks.l <- split(word.nodes, ceiling(x/max.length))
chunk.total <- append(chunk.total, length(chunks.l))
}
i <- 1
chunk.number <- NULL
ID.holder <- NULL
n <- 1
for (i in 1:length(chunk.total)) {
chunk.number <- seq_along(1:chunk.total[i])
# chunk.number corresponds to the different file names to used in row IDs
for (j in 1:length(chunk.number)) {
single.name <- paste(bookids.v[i], chunk.number[j], sep = "-", collapse = NULL)
ID.holder <- append(ID.holder, rep(single.name, nrow(freqs.l[[n]])))
n <-n+1
}
j <- 1
}
ID.holder[100000]
freqs.df$ID <- ID.holder
result.t <- xtabs(Freq ~ ID+combined.content, data=freqs.df)
dim(result.t)
final.df <- as.data.frame.matrix(result.t)
freq.means.v <- colMeans(final.df[, ])
keepers.v <- which(freq.means.v >=.0003)
smaller.df <- final.df[, keepers.v]
keepers.v <- which(freq.means.v >=.0044)
smaller.df <- final.df[, keepers.v]
dim(smaller.df)
ordered.df <- smaller.df[, order(colMeans(smaller.df), decreasing=TRUE)]
View(ordered.df)
samller.df <- ordered.df[, 1:2022]
smaller.df <- ordered.df[, 1:2022]
rm(samller.df)
write.csv(row.names(smaller.df), file = "Rresults/matrices/sWordLevels_Sample_40tokens_rowNames.csv")
chunk.total
per_author <- 1/6
per_author / 202
per_author / 260
per_author / 127
per_author / 173
per_author / 699
per_author / 200
require(e1071)
require(gmodels)
require(klaR)
chunk.ratios.m <- read.csv(file="Rresults/matrices/sWordLevels_Sample_40tokens_rowNames.csv", stringsAsFactors = FALSE, header = TRUE)
author.factor <- chunk.ratios.m[, 1]
svm.results.l <- list()
svm.error.matrix.l <- list()
testing.classes.l <- list()
i <- 1
for (i in 1:100) {
#create vector of random integers = 10% of obs in smaller.df
testing.index.v <- sample (seq (1, nrow(smaller.df)), 133, prob = chunk.ratios.m[, 2])
#create training and testing data matrices using testing.index.v and its inverse
testing.data <- smaller.df[testing.index.v, ]
training.data <- smaller.df[-testing.index.v, ]
#create vectors of factors giving classes (here = authors) of each row in testing.data and training.data
training.classes <- as.factor(author.factor[-testing.index.v])
testing.classes <- as.factor(author.factor[testing.index.v])
model.svm <- svm(training.data, training.classes, kernel = "linear", scale = FALSE)
svm.results.l[[i]] <- predict(model.svm, testing.data)
svm.error.matrix.l[[i]] <- errormatrix(testing.classes, svm.results.l[[i]])
}
a <- do.call(rbind, svm.error.matrix.l)
sum(a[,7])
(13300-(sum(a[,7])/2))/13300
svm.results.l <- list()
svm.error.matrix.l <- list()
testing.classes.l <- list()
i <- 1
for (i in 1:100) {
#create vector of random integers = 10% of obs in smaller.df
testing.index.v <- sample (seq (1, nrow(smaller.df)), 166, prob = chunk.ratios.m[, 2])
#create training and testing data matrices using testing.index.v and its inverse
testing.data <- smaller.df[testing.index.v, ]
training.data <- smaller.df[-testing.index.v, ]
#create vectors of factors giving classes (here = authors) of each row in testing.data and training.data
training.classes <- as.factor(author.factor[-testing.index.v])
testing.classes <- as.factor(author.factor[testing.index.v])
model.svm <- svm(training.data, training.classes, kernel = "linear", scale = FALSE)
svm.results.l[[i]] <- predict(model.svm, testing.data)
svm.error.matrix.l[[i]] <- errormatrix(testing.classes, svm.results.l[[i]])
}
a <- do.call(rbind, svm.error.matrix.l)
View(a)
sum(a[,7])/2
(16600-(sum(a[,7])/2))/16600
summary(lengths(sWord.freq.table.list))
# clean workspace
rm(list = ls())
source("code/corpusFunctions.R")
input.dir <- "../sWord_levels/VG_files_POS_and_REL_and_relpos"
files.v <- dir(path=input.dir, pattern=".*xml")
i <- 1
sWord.freq.table.list <- list()
# set up loop to process each file in source directory
for (i in 1:length(files.v)) {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# extract all <word> elements and children into XmlNodeList object
word.nodes <- getNodeSet(doc.object, "//word")
word.nodes <- word.nodes[sample(1:length(word.nodes), floor(length(word.nodes)/3))]
# here we must split files into chunks
divisor <- length(word.nodes)/30
max.length <- length(word.nodes)/divisor
x <- seq_along(word.nodes)
chunks.l <- split(word.nodes, ceiling(x/max.length))
# set up a loop to process each chunk produced by preceding code in matrix
# set up list to store results of processsing
sWord.nodes.l <- list()
# extract sWord elements from each chunk
for (k in 1:length(chunks.l)) {
sWord.nodes.l[[k]] <- sapply(chunks.l[[k]], xmlChildren)
# create vector object to hold results
combined.content <- NULL
# set increment counter to 1
m <- 1
for (m in 1:length(sWord.nodes.l[[k]])) {
# extract sWord nodes for 1 token at a tiime; result is an XML list object with content which
# includes sWord xml tags.
single.token <- sWord.nodes.l[[k]][m]
# strip sWord level xml tags from data; result is a xml list object
content.nodes <- sapply(single.token[[1]], xmlChildren)
# create vector for output
extracted.content.v <- NULL
# set increment counter to 1
n <- 1
# iterate loop through each successive content.nodes object produced by matrix loop
for (n in 1:length(content.nodes)) {
# add successively extracted content to vector
extracted.content.v <- append(extracted.content.v, xmlValue(content.nodes[[n]]))
}
# collect output of nested loop in vector of all sWords for each token in chunk
combined.content <- append(combined.content, extracted.content.v)
}
# change sWord.contents vector to lower case
combined.content <- tolower(combined.content)
# create a contingency table of sWord.contents. The table lists nuber of occurences for all sWords.
sWord.table <- table(combined.content)
# normalize sWord.table by dividing by total sWords. Multiply by 100 to produce rate of sWord occurence per 100 sWords.
sWord.freq.table <- 100*(sWord.table/sum(sWord.table))
# insert sWord.freq.table into list
sWord.freq.table.list[[length(sWord.freq.table.list)+1]] <- sWord.freq.table
}
k <- 1
}
word.nodes[1]
word.nodes[3331]
sWord.nodes.l[[1]]
sWord.nodes.l[[1]][1]
length(sWord.nodes.l)
word.nodes[112]
word.nodes[113]
rm(list = ls())
source("code/corpusFunctions.R")
input.dir <- "../sWord_levels/VG_files_POS_and_REL_and_relpos"
files.v <- dir(path=input.dir, pattern=".*xml")
i <- 1
# create list object with no content. Vectors extracted from XML files will be stored here.
sWord.freq.table.list <- list()
# set up loop to process each file in source directory
for (i in 1:length(files.v)) {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# extract all <word> elements and children into XmlNodeList object
word.nodes <- getNodeSet(doc.object, "//word")
word.nodes <- word.nodes[sample(1:length(word.nodes), floor(length(word.nodes)/3))]
# here we must split files into chunks
divisor <- length(word.nodes)/30
max.length <- length(word.nodes)/divisor
x <- seq_along(word.nodes)
chunks.l <- split(word.nodes, ceiling(x/max.length))
# set up a loop to process each chunk produced by preceding code in matrix
# set up list to store results of processsing
sWord.nodes.l <- list()
# extract sWord elements from each chunk
for (k in 1:length(chunks.l)) {
sWord.nodes.l[[k]] <- sapply(chunks.l[[k]], xmlChildren)
# create vector object to hold results
combined.content <- NULL
# set increment counter to 1
m <- 1
for (m in 1:length(sWord.nodes.l[[k]])) {
# extract sWord nodes for 1 token at a tiime; result is an XML list object with content which
# includes sWord xml tags.
single.token <- sWord.nodes.l[[k]][m]
# strip sWord level xml tags from data; result is a xml list object
content.nodes <- sapply(single.token[[1]], xmlChildren)
# create vector for output
extracted.content.v <- NULL
# set increment counter to 1
n <- 1
# iterate loop through each successive content.nodes object produced by matrix loop
for (n in 1:length(content.nodes)) {
# add successively extracted content to vector
extracted.content.v <- append(extracted.content.v, xmlValue(content.nodes[[n]]))
}
# collect output of nested loop in vector of all sWords for each token in chunk
combined.content <- append(combined.content, extracted.content.v)
}
# change sWord.contents vector to lower case
combined.content <- tolower(combined.content)
# create a contingency table of sWord.contents. The table lists nuber of occurences for all sWords.
sWord.table <- table(combined.content)
# normalize sWord.table by dividing by total sWords. Multiply by 100 to produce rate of sWord occurence per 100 sWords.
sWord.freq.table <- 100*(sWord.table/sum(sWord.table))
# insert sWord.freq.table into list
sWord.freq.table.list[[length(sWord.freq.table.list)+1]] <- sWord.freq.table
}
k <- 1
}
word.nodes[1]
sWord.nodes.l[[1]][1]
length(sWord.nodes.l)
lengths(sWord.nodes.l)
sWord.nodes.l[[1]][6]
word.nodes[112]
word.nodes[113]
n <- 1
# iterate loop through each successive content.nodes object produced by matrix loop
for (n in 1:length(content.nodes)) {
# add successively extracted content to vector
extracted.content.v <- append(extracted.content.v, xmlValue(content.nodes[[n]]))
}
extracted.content.v <- tolower(extracted.content.v)
rm(list = ls())
source("code/corpusFunctions.R")
input.dir <- "../sWord_levels/VG_files_POS_and_REL_and_relpos"
files.v <- dir(path=input.dir, pattern=".*xml")
i <- 1
sWord.freq.table.list <- list()
# set up loop to process each file in source directory
for (i in 1:length(files.v)) {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# extract all <word> elements and children into XmlNodeList object
word.nodes <- getNodeSet(doc.object, "//word")
word.nodes <- word.nodes[sample(1:length(word.nodes), floor(length(word.nodes)/3))]
# here we must split files into chunks
divisor <- length(word.nodes)/25
max.length <- length(word.nodes)/divisor
x <- seq_along(word.nodes)
chunks.l <- split(word.nodes, ceiling(x/max.length))
# set up a loop to process each chunk produced by preceding code in matrix
# set up list to store results of processsing
sWord.nodes.l <- list()
# extract sWord elements from each chunk
for (k in 1:length(chunks.l)) {
sWord.nodes.l[[k]] <- sapply(chunks.l[[k]], xmlChildren)
# create vector object to hold results
combined.content <- NULL
# set increment counter to 1
m <- 1
for (m in 1:length(sWord.nodes.l[[k]])) {
# extract sWord nodes for 1 token at a tiime; result is an XML list object with content which
# includes sWord xml tags.
single.token <- sWord.nodes.l[[k]][m]
# strip sWord level xml tags from data; result is a xml list object
content.nodes <- sapply(single.token[[1]], xmlChildren)
# create vector for output
extracted.content.v <- NULL
# set increment counter to 1
n <- 1
# iterate loop through each successive content.nodes object produced by matrix loop
for (n in 1:length(content.nodes)) {
# add successively extracted content to vector
extracted.content.v <- append(extracted.content.v, xmlValue(content.nodes[[n]]))
extracted.content.v <- tolower(extracted.content.v)
}
# collect output of nested loop in vector of all sWords for each token in chunk
combined.content <- append(combined.content, extracted.content.v)
}
# change sWord.contents vector to lower case
combined.content <- tolower(combined.content)
# create a contingency table of sWord.contents. The table lists nuber of occurences for all sWords.
sWord.table <- table(combined.content)
# normalize sWord.table by dividing by total sWords. Multiply by 100 to produce rate of sWord occurence per 100 sWords.
sWord.freq.table <- 100*(sWord.table/sum(sWord.table))
# insert sWord.freq.table into list
sWord.freq.table.list[[length(sWord.freq.table.list)+1]] <- sWord.freq.table
}
k <- 1
}
length(sWord.freq.table.list)
lengths(sWord.freq.table.list)
summary(lengths(sWord.freq.table.list))
i <- 1
freqs.l <- list()
for (i in 1:length(sWord.freq.table.list)) {
freqs.l[[i]] <-data.frame(sWord.freq.table.list[[i]], ID=seq_along(sWord.freq.table.list[[i]]),
stringsAsFactors=FALSE )
}
freqs.df <- do.call(rbind, freqs.l)
bookids.v <- gsub(".xml", "", files.v)
chunk.total <- NULL
i <- 1
for (i in 1:length(files.v))  {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# extract all <word> elements and children into XmlNodeList object
word.nodes <- getNodeSet(doc.object, "//word")
word.nodes <- word.nodes[sample(1:length(word.nodes), floor(length(word.nodes)/3))]
# here we must split files into chunks
divisor <- length(word.nodes)/25
max.length <- length(word.nodes)/divisor
x <- seq_along(word.nodes)
chunks.l <- split(word.nodes, ceiling(x/max.length))
chunk.total <- append(chunk.total, length(chunks.l))
}
i <- 1
chunk.number <- NULL
ID.holder <- NULL
n <- 1
for (i in 1:length(chunk.total)) {
chunk.number <- seq_along(1:chunk.total[i])
# chunk.number corresponds to the different file names to used in row IDs
for (j in 1:length(chunk.number)) {
single.name <- paste(bookids.v[i], chunk.number[j], sep = "-", collapse = NULL)
ID.holder <- append(ID.holder, rep(single.name, nrow(freqs.l[[n]])))
n <-n+1
}
j <- 1
}
ID.holder[100000]
freqs.df$ID <- ID.holder
result.t <- xtabs(Freq ~ ID+combined.content, data=freqs.df)
dim(result.t)
final.df <- as.data.frame.matrix(result.t)
freq.means.v <- colMeans(final.df[, ])
keepers.v <- which(freq.means.v >=.0044)
smaller.df <- final.df[, keepers.v]
ordered.df <- smaller.df[, order(colMeans(smaller.df), decreasing=TRUE)]
smaller.df <- ordered.df[, 1:2022]
write.csv(row.names(smaller.df), file = "Rresults/matrices/sWordLevels_Sample_25tokens_rowNames.csv")
chunk.total
per_author <- 1/6
per_author / 323
per_author / 416
per_author / 204
per_author / 276
per_author / 1114
per_author / 319
chunk.ratios.m <- read.csv(file="Rresults/matrices/sWordLevels_Sample_25tokens_rowNames.csv", stringsAsFactors = FALSE, header = TRUE)
author.factor <- chunk.ratios.m[, 1]
svm.results.l <- list()
svm.error.matrix.l <- list()
testing.classes.l <- list()
i <- 1
i <- 1
for (i in 1:100) {
#create vector of random integers = 10% of obs in smaller.df
testing.index.v <- sample (seq (1, nrow(smaller.df)), 260, prob = chunk.ratios.m[, 2])
#create training and testing data matrices using testing.index.v and its inverse
testing.data <- smaller.df[testing.index.v, ]
training.data <- smaller.df[-testing.index.v, ]
#create vectors of factors giving classes (here = authors) of each row in testing.data and training.data
training.classes <- as.factor(author.factor[-testing.index.v])
testing.classes <- as.factor(author.factor[testing.index.v])
model.svm <- svm(training.data, training.classes, kernel = "linear", scale = FALSE)
svm.results.l[[i]] <- predict(model.svm, testing.data)
svm.error.matrix.l[[i]] <- errormatrix(testing.classes, svm.results.l[[i]])
}
a <- do.call(rbind, svm.error.matrix.l)
View(a)
chunk.ratios.m <- read.csv(file="Rresults/matrices/sWordLevels_Sample_25tokens_rowNames.csv", stringsAsFactors = FALSE, header = TRUE)
author.factor <- chunk.ratios.m[, 1]
unique(author.factor)
svm.results.l <- list()
svm.error.matrix.l <- list()
testing.classes.l <- list()
i <- 1
for (i in 1:100) {
#create vector of random integers = 10% of obs in smaller.df
testing.index.v <- sample (seq (1, nrow(smaller.df)), 260, prob = chunk.ratios.m[, 2])
#create training and testing data matrices using testing.index.v and its inverse
testing.data <- smaller.df[testing.index.v, ]
training.data <- smaller.df[-testing.index.v, ]
#create vectors of factors giving classes (here = authors) of each row in testing.data and training.data
training.classes <- as.factor(author.factor[-testing.index.v])
testing.classes <- as.factor(author.factor[testing.index.v])
model.svm <- svm(training.data, training.classes, kernel = "linear", scale = FALSE)
svm.results.l[[i]] <- predict(model.svm, testing.data)
svm.error.matrix.l[[i]] <- errormatrix(testing.classes, svm.results.l[[i]])
}
a <- do.call(rbind, svm.error.matrix.l)
View(a)
sum(a[,7])/2
(26000-(sum(a[,7])/2))/26000
summary(lengths(sWord.freq.table.list))
