}
j <- 1
}
ID.holder[384:387]
ID.holder[425124]
ID.holder <- NULL
i <- 1
single.name <- NULL
for (i in 1:length(bookids.v)) {
for (j in 1:length(freqs.l[[i]]$ID)) {
single.name <- paste(bookids.v[i], freqs.l[[i]]$ID[j], sep = "-", collapse = NULL)
ID.holder <- append(ID.holder, single.name)
}
z <-  2*i
j <- 1
}
freqs.l[[1]]$ID
chunk.total <- NULL
i <- 1
for (i in 1:length(files.v))  {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# extract all <word> elements and children into XmlNodeList object
word.nodes <- getNodeSet(doc.object, "//word")
# here we must split files into chunks
divisor <- length(word.nodes)/1000
max.length <- length(word.nodes)/divisor
x <- seq_along(word.nodes)
chunks.l <- split(word.nodes, ceiling(x/max.length))
chunk.total <- append(chunk.total, length(chunks.l))
}
i <- 1
chunk.number <- NULL
ID.holder <- NULL
n <- 1
# note that this code does not work correctly !!
for (i in 1:length(chunk.total)) {
chunk.number <- seq_along(1:chunk.total[i])
# chunk.number corresponds to the different file names to used in row IDs
for (j in 1:length(chunk.number)) {
single.name <- paste(bookids.v[i], chunk.number[j], sep = "-", collapse = NULL)
ID.holder <- append(ID.holder, rep(single.name, nrow(freqs.l[[n]])))
n <-n+1
}
j <- 1
}
ID.holder[425124]
ID.holder[100000]
freqs.df2$ID <- ID.holder
freqs.df$ID <- ID.holder
result.t <- xtabs(Freq ~ ID+combined.content, data=freqs.df2)
result.t <- xtabs(Freq ~ ID+combined.content, data=freqs.df)
dim(result.t)
final.df <- as.data.frame.matrix(result.t)
rm(result.t)
freq.means.v <- colMeans(final.df[, ])
keepers.v <- which(freq.means.v >=.003)
keepers.v <- which(freq.means.v >=.03)
keepers.v <- which(freq.means.v >=.04)
keepers.v <- which(freq.means.v >=.08)
keepers.v <- which(freq.means.v >=.09)
smaller.df <- final.df[, keepers.v]
dim(smaller.df)
chunk.authors.m <- read.csv(file = "Rresults/matrices/chunk_ratios_100.csv")
chunk.authors.m <- read.csv(file = "Rresults/matrices/SuthorFactor_1000tokenChunks_June-13-2016.csv.csv", stringsAsFactors = FALSE)
chunk.authors.m <- read.csv(file = "Rresults/matrices/SuthorFactor_1000tokenChunks_June-13-2016.csv", stringsAsFactors = FALSE)
chunk.authors.m <- read.csv(file = "Rresults/matrices/AuthorFactor_1000tokenChunks_June-13-2016.csv", stringsAsFactors = FALSE)
author.v <- chunk.authors.m[, 2]
probs.m <- read.csv(file = "Rresults/matrices/chunk_ratios_1000.csv", stringsAsFactors = FALSE)
testing.index <- sample(1:175, 18, prob = probs.m[, 2])
testing.data <- smaller.df[testing.index, ]
training.data <- smaller.df[-testing.data, ]
training.data <- smaller.df[-testing.index, ]
testing.classes <- as.factor(author.v[testing.index])
training.classes <- as.factor(author.v[-testing.index])
svm.model <- svm(training.data, training.classes, kernel = "linear", scale = TRUE)
svm.results <- predict(svm.model, testing.data)
svm.results
svm.error.matrix.l <- list()
i <- 1
for (i in 1:100) {
testing.index <- sample(1:175, 18, prob = probs.m[, 2])
testing.data <- smaller.df[testing.index, ]
training.data <- smaller.df[-testing.index, ]
testing.classes <- as.factor(author.v[testing.index])
training.classes <- as.factor(author.v[-testing.index])
svm.model <- svm(training.data, training.classes, kernel = "linear", scale = TRUE)
svm.results <- predict(svm.model, testing.data)
svm.error.matrix.l[[i]] <- errormatrix(testing.classes, svm.results.l[[i]])
}
svm.results.l <- list()
for (i in 1:100) {
testing.index <- sample(1:175, 18, prob = probs.m[, 2])
testing.data <- smaller.df[testing.index, ]
training.data <- smaller.df[-testing.index, ]
testing.classes <- as.factor(author.v[testing.index])
training.classes <- as.factor(author.v[-testing.index])
svm.model <- svm(training.data, training.classes, kernel = "linear", scale = TRUE)
svm.results.l[[i]] <- predict(svm.model, testing.data)
svm.error.matrix.l[[i]] <- errormatrix(testing.classes, svm.results.l[[i]])
}
a <- do.call(rbind, svm.error.matrix.l)
View(a)
sum(a[, 6])
(1800- (sum(a[, 6])/2))/1800
write.csv(a, file="Rresults/svmError_matrix__sWordLevels_TrueScale_June-14-2016.csv")
i <- 1
svm.results.l <- list()
svm.error.matrix.l <- list()
testing.classes.l <- list()
i <- 1
for (i in 1:100) {
testing.index <- sample(1:175, 18, prob = probs.m[, 2])
testing.data <- smaller.df[testing.index, ]
training.data <- smaller.df[-testing.index, ]
testing.classes <- as.factor(author.v[testing.index])
training.classes <- as.factor(author.v[-testing.index])
svm.model <- svm(training.data, training.classes, kernel = "linear", scale = FALSE)
svm.results.l[[i]] <- predict(svm.model, testing.data)
svm.error.matrix.l[[i]] <- errormatrix(testing.classes, svm.results.l[[i]])
}
a <- do.call(rbind, svm.error.matrix.l)
View(a)
sum(a[, 6])
(1800- (sum(a[, 6])/2))/1800
dim(smaller.df)
keepers.v <- which(freq.means.v >=.009)
smaller.df <- final.df[, keepers.v]
write.csv(a, file="Rresults/svmError_matrix__sWordLevels_FalseScale_June-14-2016.csv")
svm.results.l <- list()
svm.error.matrix.l <- list()
testing.classes.l <- list()
i <- 1
svm.results.l <- list()
svm.error.matrix.l <- list()
testing.classes.l <- list()
i <- 1
for (i in 1:100) {
testing.index <- sample(1:175, 18, prob = probs.m[, 2])
testing.data <- smaller.df[testing.index, ]
training.data <- smaller.df[-testing.index, ]
testing.classes <- as.factor(author.v[testing.index])
training.classes <- as.factor(author.v[-testing.index])
svm.model <- svm(training.data, training.classes, kernel = "linear", scale = FALSE)
svm.results.l[[i]] <- predict(svm.model, testing.data)
svm.error.matrix.l[[i]] <- errormatrix(testing.classes, svm.results.l[[i]])
}
a <- do.call(rbind, svm.error.matrix.l)
sum(a[, 6])
(1800- (sum(a[, 6])/2))/1800
svm.results.l <- list()
svm.error.matrix.l <- list()
testing.classes.l <- list()
i <- 1
write.csv(a, file="Rresults/svmError_matrix__sWordLevels_FalseScale_1006Obs__June-14-2016.csv")
svm.results.l <- list()
svm.error.matrix.l <- list()
testing.classes.l <- list()
i <- 1
for (i in 1:100) {
testing.index <- sample(1:175, 18, prob = probs.m[, 2])
testing.data <- smaller.df[testing.index, ]
training.data <- smaller.df[-testing.index, ]
testing.classes <- as.factor(author.v[testing.index])
training.classes <- as.factor(author.v[-testing.index])
svm.model <- svm(training.data, training.classes, kernel = "linear", scale = TRUE)
svm.results.l[[i]] <- predict(svm.model, testing.data)
svm.error.matrix.l[[i]] <- errormatrix(testing.classes, svm.results.l[[i]])
}
a <- do.call(rbind, svm.error.matrix.l)
sum(a[, 6])
(1800- (sum(a[, 6])/2))/1800
write.csv(a, file="Rresults/svmError_matrix__sWordLevels_TrueScale_1006Obs__June-14-2016.csv")
rm(list = ls())
source("code/corpusFunctions.R")
input.dir <- "../sWord_levels/files_with_sWords/VG_files"
files.v <- dir(path=input.dir, pattern=".*xml")
book.freqs.l <- list()
for(i in 1:length(files.v)){
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
chunk.data.l <- getSwordChunkMaster(doc.object, 1000)
book.freqs.l[[files.v[i]]] <-chunk.data.l
}
rm(list = ls())
source("code/corpusFunctions.R")
input.dir <- "../sWord_levels/files_with_sWords/VG_files"
files.v <- dir(path=input.dir, pattern=".*xml")
i <- 1
sWord.freq.table.list <- list()
geodesic.freq.table.list <- list()
[for (i in 1:length(files.v)) {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# input all <word> elements into object "words"
words <- getNodeSet(doc.object, "//word")
# input all geodesics (i.e., full sWords from leaf to root) into object "geodesics".
geodesics <- getNodeSet(doc.object, "//word/*[last()]")
# extract content of geodesic  elements and put it into a character vector object
geodesic.content <- paste(sapply(geodesics, xmlValue), collapse=NULL)
# convert geodesic.content to lower case
geodesic.content <- tolower(geodesic.content)
# here we  split object "words" into chunks;
divisor <- length(words)/1000 # this divisor indicates size of word chunks; the result it the real number representing
# the number of x-length chunks in "words"
# a wrapper for use in ceiling(); it is the same as the integer supplied above.
max.length <- length(words)/divisor
# create a vector of integers corresponding to each <word> element in "words"
x <- seq_along(words)
# create a list object of elements each with no more than selected chunk size of elements (the last set in list will be smaller
# unless the number of words in file is evenly divisible). The function ceililng() is used to ensure an integer for the argument of
# the function split().
chunks.l <- split(words, ceiling(x/max.length))
# create a list object of for chunks of geodesics; the same arguments may be used for function split() as were used for
# <word> elements because there is one geodesic per <word> and the arguments should thus be the same.
geodesic.chunks.l <- split(geodesic.content, ceiling(x/max.length))
# the following loop extracts content of the children of the <word> elements
# set up list to store results of processsing
sWord.nodes.l <- list()
for (j in 1:length(chunks.l)) {
sWord.nodes.l[[j]] <- sapply(chunks.l[[j]], xmlChildren)
# create vector object to hold results
combined.content <- NULL
# set increment counter to 1
m <- 1
for (m in 1:length(sWord.nodes.l[[j]])) {
# extract sWord nodes for 1 token at a tiime; result is an XML list object with content which
# includes sWord xml tags.
single.token <- sWord.nodes.l[[j]][m]
# strip sWord level xml tags from data; result is a xml list object
content.nodes <- sapply(single.token[[1]], xmlChildren)
# create vector for output
extracted.content.v <- NULL
# set increment counter to 1
n <- 1
# iterate loop through each successive content.nodes object produced by matrix loop
for (n in 1:length(content.nodes)) {
# add successively extracted content to vector
extracted.content.v <- append(extracted.content.v, xmlValue(content.nodes[[n]]))
} # end of loop n
# collect output of nested loop in vector of all sWords for each token in chunk
combined.content <- append(combined.content, extracted.content.v)
} # end of loop m
# change sWord.contents vector to lower case
combined.content <- tolower(combined.content)
# create a contingency table of sWord.contents. The table lists nuber of occurences for all sWords.
sWord.table <- table(combined.content)
# normalize sWord.table by dividing by total sWords. Multiply by 100 to produce rate of sWord occurence per 100 sWords.
sWord.freq.table <- 100*(sWord.table/sum(sWord.table))
# insert sWord.freq.table into list
sWord.freq.table.list[[length(sWord.freq.table.list)+1]] <- sWord.freq.table
} # end of loop j
# reset increment for loop
j <- 1
# begin loop "k" to hadle geodesic chunks.
for (k in 1:length(geodesic.chunks.l)) {
#convert the list object geodesic.chunks.l into contigency table objects
geodesic.table <- table(geodesic.chunks.l[[k]])
#convert the result from raw frequencies to relative fequencies
geodesic.rel.freq.table <- prop.table(geodesic.table)
# store output in list
geodesic.freq.table.list[[length(geodesic.freq.table.list)+1]] <- geodesic.rel.freq.table
} # end of loop k
# reset increment for loop k
k <- 1
} # end of loop i
rm(list = ls())
input.dir <- "../sWord_levels/files_with_sWords/VG_files"
files.v <- dir(path=input.dir, pattern=".*xml")
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
i <- 1
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
geodesics <- getNodeSet(doc.object, "//word/*[last()]")
geodesic.content <- paste(sapply(geodesics, xmlValue), collapse=NULL)
geodesic.content <- tolower(geodesic.content)
divisor <- length(geodesics)/1000 # this divisor indicates size of word chunks; the result it the real number representing
max.length <- length(geodesics)/divisor
x <- seq_along(geodesics)
chunks.l <- split(geodesics, ceiling(x/max.length))
chunks.l <- split(geodesics.content, ceiling(x/max.length))
chunks.l <- split(geodesic.content, ceiling(x/max.length))
geodesic.table <- lapply(geodesic.chunks.l, table)
geodesic.table <- lapply(chunks.l, table)
freq.table <- lapply(geodesic.table, prop.table)
rm(list = ls())
input.dir <- "../sWord_levels/files_with_sWords/VG_files"
files.v <- dir(path=input.dir, pattern=".*xml")
freq.table <- list()
i <- 1
for (i in 1: length(files.v)) {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# input all geodesics (i.e., full sWords from leaf to root) into object "geodesics".
geodesics <- getNodeSet(doc.object, "//word/*[last()]")
# extract content of geodesic  elements and put it into a character vector object
geodesic.content <- paste(sapply(geodesics, xmlValue), collapse=NULL)
# convert geodesic.content to lower case
geodesic.content <- tolower(geodesic.content)
# here we  split object "words" into chunks;
divisor <- length(geodesics)/1000 # this divisor indicates size of word chunks; the result it the real number representing
# the number of x-length chunks in "words"
# a wrapper for use in ceiling(); it is the same as the integer supplied above.
max.length <- length(geodesics)/divisor
# create a vector of integers corresponding to each <word> element in "words"
x <- seq_along(geodesics)
# create a list object of elements each with no more than selected chunk size of elements (the last set in list will be smaller
# unless the number of words in file is evenly divisible). The function ceililng() is used to ensure an integer for the argument of
# the function split().
chunks.l <- split(geodesic.content, ceiling(x/max.length))
# create contingincy table; this device records the number of occurence of each type of sWord in each chunk
geodesic.table <- lapply(chunks.l, table)
# create relative frequency table
freq.table <- lapply(geodesic.table, prop.table)
# store output of loop
freq.table.l[[i]] <- freq.table
}
freq.table.l <- list()
rm(list = ls())
input.dir <- "../sWord_levels/files_with_sWords/VG_files"
files.v <- dir(path=input.dir, pattern=".*xml")
freq.table.l <- list()
i <- 1
for (i in 1: length(files.v)) {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# input all geodesics (i.e., full sWords from leaf to root) into object "geodesics".
geodesics <- getNodeSet(doc.object, "//word/*[last()]")
# extract content of geodesic  elements and put it into a character vector object
geodesic.content <- paste(sapply(geodesics, xmlValue), collapse=NULL)
# convert geodesic.content to lower case
geodesic.content <- tolower(geodesic.content)
# here we  split object "words" into chunks;
divisor <- length(geodesics)/1000 # this divisor indicates size of word chunks; the result it the real number representing
# the number of x-length chunks in "words"
# a wrapper for use in ceiling(); it is the same as the integer supplied above.
max.length <- length(geodesics)/divisor
# create a vector of integers corresponding to each <word> element in "words"
x <- seq_along(geodesics)
# create a list object of elements each with no more than selected chunk size of elements (the last set in list will be smaller
# unless the number of words in file is evenly divisible). The function ceililng() is used to ensure an integer for the argument of
# the function split().
chunks.l <- split(geodesic.content, ceiling(x/max.length))
# create contingincy table; this device records the number of occurence of each type of sWord in each chunk
geodesic.table <- lapply(chunks.l, table)
# create relative frequency table
freq.table <- lapply(geodesic.table, prop.table)
# store output of loop
freq.table.l[[i]] <- freq.table
}
i <- 1
freqs.l <- list()
for (i in 1:length(sWord.freq.table.list)) {
freqs.l[[i]] <-data.frame(freq.table.l[[i]], ID=seq_along(freq.table.list[[i]]),
stringsAsFactors=FALSE )
}
i <- 1
for (i in 1:length(freq.table.list)) {
freqs.l[[i]] <-data.frame(freq.table.l[[i]], ID=seq_along(freq.table.list[[i]]),
stringsAsFactors=FALSE )
}
for (i in 1:length(freq.table.l)) {
freqs.l[[i]] <-data.frame(freq.table.l[[i]], ID=seq_along(freq.table.list[[i]]),
stringsAsFactors=FALSE )
}
for (i in 1:length(freq.table.l)) {
freqs.l[[i]] <-data.frame(freq.table.l[[i]], ID=seq_along(freq.table.l[[i]]),
stringsAsFactors=FALSE )
}
data.frame(freq.table.l[[i]], ID=seq_along(freq.table.l[[i]]),
stringsAsFactors=FALSE )
data.frame(freq.table.l[[i]][1], ID=seq_along(freq.table.l[[i]]),
stringsAsFactors=FALSE )
data.frame(freq.table.l[[i]][1], ID=seq_along(freq.table.l[[i]]),
stringsAsFactors=FALSE )
data.frame(freq.table.l[[i]][1], ID=seq_along(freq.table.l[[i]]),
stringsAsFactors=FALSE, check.rows = FALSE )
data.frame(freq.table.l[[i]][1], ID=seq_along(freq.table.l[[i]]),
stringsAsFactors=FALSE, SIMPLIFY = FALSE )
freqs.l <- lapply(freq.table.l, my.apply)
my.apply <- function(x){
my.list <-mapply(data.frame, ID=seq_along(x),
x, SIMPLIFY=FALSE,
MoreArgs=list(stringsAsFactors=FALSE))
my.df <- do.call(rbind, my.list)
return(my.df)
}
freqs.l <- lapply(freq.table.l, my.apply)
freqs.df <- do.call(rbind, freqs.l)
bookids.v <- gsub(".xml", "", files.v)
chunk.total <- NULL
i <- 1
for (i in 1:length(files.v))  {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# extract all <word> elements and children into XmlNodeList object
word.nodes <- getNodeSet(doc.object, "//word")
# here we must split files into chunks
divisor <- length(word.nodes)/1000
max.length <- length(word.nodes)/divisor
x <- seq_along(word.nodes)
chunks.l <- split(word.nodes, ceiling(x/max.length))
chunk.total <- append(chunk.total, length(chunks.l))
}
i <- 1
chunk.number <- NULL
ID.holder <- NULL
n <- 1
for (i in 1:length(chunk.total)) {
chunk.number <- seq_along(1:chunk.total[i])
# chunk.number corresponds to the different file names to used in row IDs
for (j in 1:length(chunk.number)) {
single.name <- paste(bookids.v[i], chunk.number[j], sep = "-", collapse = NULL)
ID.holder <- append(ID.holder, rep(single.name, nrow(sWord.freqs.l[[n]])))
n <-n+1
}
j <- 1
}
i <- 1
chunk.number <- NULL
ID.holder <- NULL
n <- 1
for (i in 1:length(chunk.total)) {
chunk.number <- seq_along(1:chunk.total[i])
# chunk.number corresponds to the different file names to used in row IDs
for (j in 1:length(chunk.number)) {
single.name <- paste(bookids.v[i], chunk.number[j], sep = "-", collapse = NULL)
ID.holder <- append(ID.holder, rep(single.name, nrow(freqs.l[[n]])))
n <-n+1
}
j <- 1
}
ID.holder <- NULL
i <- 1
single.name <- NULL
freqs.l[[1]]$ID
for (i in 1:length(bookids.v)) {
for (j in 1:length(freqs.l[[i]]$ID)) {
single.name <- paste(bookids.v[i], freqs.l[[i]]$ID[j], sep = "-", collapse = NULL)
ID.holder <- append(ID.holder, single.name)
}
z <-  2*i
j <- 1
}
freqs.df$ID <- ID.holder
result.t <- xtabs(Freq ~ ID+combined.content, data=sWord.freqs.df)
result.t <- xtabs(Freq ~ ID+combined.content, data=freqs.df)
result.t <- xtabs(Freq ~ ID+Var1, data=freqs.df)
dim(result.t)
final.df <- as.data.frame.matrix(result.t)
rm(result.t)
freq.means.v <- colMeans(final.df[, ])
keepers.v <- which(freq.means.v >=.003)
keepers.v <- which(freq.means.v >=.0003)
keepers.v <- which(freq.means.v >=.00003)
keepers.v <- which(freq.means.v >=.00009)
keepers.v <- which(freq.means.v >=.00008)
smaller.df <- final.df[, keepers.v]
chunk.authors.m <- read.csv(file = "Rresults/matrices/AuthorFactor_1000tokenChunks_June-13-2016.csv", stringsAsFactors = FALSE)
author.v <- chunk.authors.m[, 2]
probs.m <- read.csv(file = "Rresults/matrices/chunk_ratios_1000.csv", stringsAsFactors = FALSE)
svm.results.l <- list()
svm.error.matrix.l <- list()
testing.classes.l <- list()
i <- 1
for (i in 1:100) {
testing.index <- sample(1:175, 18, prob = probs.m[, 2])
testing.data <- smaller.df[testing.index, ]
training.data <- smaller.df[-testing.index, ]
testing.classes <- as.factor(author.v[testing.index])
training.classes <- as.factor(author.v[-testing.index])
svm.model <- svm(training.data, training.classes, kernel = "linear", scale = TRUE)
svm.results.l[[i]] <- predict(svm.model, testing.data)
svm.error.matrix.l[[i]] <- errormatrix(testing.classes, svm.results.l[[i]])
}
a <- do.call(rbind, svm.error.matrix.l)
sum(a[, 6])
(1800- (sum(a[, 6])/2))/1800
write.csv(a, file="Rresults/svmError_matrix__sWordGeodesics_TrueScale_1091Obs__June-14-2016.csv")
svm.results.l <- list()
svm.error.matrix.l <- list()
testing.classes.l <- list()
i <- 1
for (i in 1:100) {
testing.index <- sample(1:175, 18, prob = probs.m[, 2])
testing.data <- smaller.df[testing.index, ]
training.data <- smaller.df[-testing.index, ]
testing.classes <- as.factor(author.v[testing.index])
training.classes <- as.factor(author.v[-testing.index])
svm.model <- svm(training.data, training.classes, kernel = "linear", scale = FALSE)
svm.results.l[[i]] <- predict(svm.model, testing.data)
svm.error.matrix.l[[i]] <- errormatrix(testing.classes, svm.results.l[[i]])
}
a <- do.call(rbind, svm.error.matrix.l)
sum(a[, 6])
(1800- (sum(a[, 6])/2))/1800
svm.results.l <- list()
svm.error.matrix.l <- list()
testing.classes.l <- list()
i <- 1
for (i in 1:100) {
testing.index <- sample(1:175, 18, prob = probs.m[, 2])
testing.data <- smaller.df[testing.index, ]
training.data <- smaller.df[-testing.index, ]
testing.classes <- as.factor(author.v[testing.index])
training.classes <- as.factor(author.v[-testing.index])
svm.model <- svm(training.data, training.classes, kernel = "linear", scale = TRUE)
svm.results.l[[i]] <- predict(svm.model, testing.data)
svm.error.matrix.l[[i]] <- errormatrix(testing.classes, svm.results.l[[i]])
}
a <- do.call(rbind, svm.error.matrix.l)
sum(a[, 6])
(1800- (sum(a[, 6])/2))/1800
write.csv(a, file="Rresults/svmError_matrixB__sWordGeodesics_TrueScale_1091Obs__June-14-2016.csv")
