2*2
install.packages(c("XML", "textreuse"))
p <- 29.46
i <- 24.57
r <- 250
r - (p+1)
250-54.03
195.97 + 25.23
62*31
172/15
30+420
372+72
315+121
172+220+50
a <- 15
b <- 16
c <- 15
d <- 15
a * 12
a * 15
b * 20
b * 8
c * 25
c * 5
d * 2
d * 28
16*15
15*17
15*19
seq(1:150)
f <- seq(1:150)
length(f)
b * 20
mast <- 15*17
march <- 15*2
seq(301, 546)
length(seq(301, 546))
a * 12
a <- 15
b <- 16
c <- 15
d <- 15
a * 12
card <-  seq(1101, 1172)
card <-  seq(1001, 1172)
length(card)
card <-  seq(2001, 2219)
length(card)
54.03-250
195.97+31.43
7/16
14/16
12/16
10/16
5/16
9/16
19/23
20/22
19/21
12/19
15/19
20/21
18/21
17/19
18/22
21/23
18/22
15/17
17/21
18/22
14/15
11/14
12/14
13/15
10/14
20/23
11/14
12/15
14/15
12/14
11/14
11/14
17/23
10/14
19*60
17*60
60*9
1.309 * 200
3*1.5
p <- 32.1
i <- 21.93
p + i
250- (p+i)
195.97 + p
library(XML)
source("code/corpusFunctions.R")
input.dir <- "./working_input3"
files.v <- dir(path=input.dir, pattern=".*xml")
setwd("~/syntacto_stylistics/R_files")
source("code/corpusFunctions.R")
input.dir <- "./working_input3"
files.v <- dir(path=input.dir, pattern=".*xml")
i <- 1
# create list object with no content. Vectors extracted from XML files will be stored here.
sWord.freq.table.list <- list()
:length(files.v)) {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# extract all <word> elements and children into XmlNodeList object
word.nodes <- getNodeSet(doc.object, "//word")
# here we must split files into chunks
divisor <- length(word.nodes)/98
max.length <- length(word.nodes)/divisor
x <- seq_along(word.nodes)
chunks.l <- split(word.nodes, ceiling(x/max.length))
# set up a loop to process each chunk produced by preceding code in matrix
# set up list to store results of processsing
sWord.nodes.l <- list()
# extract sWord elements from each chunk
for (k in 1:length(chunks.l)) {
sWord.nodes.l[[k]] <- sapply(chunks.l[[k]], xmlChildren)
# create vector object to hold results
combined.content <- NULL
# set increment counter to 1
m <- 1
for (m in 1:length(sWord.nodes.l[[k]])) {
# extract sWord nodes for 1 token at a tiime; result is an XML list object with content which
# includes sWord xml tags.
single.token <- sWord.nodes.l[[k]][m]
# strip sWord level xml tags from data; result is a xml list object
content.nodes <- sapply(single.token[[1]], xmlChildren)
# create vector for output
extracted.content.v <- NULL
# set increment counter to 1
n <- 1
# iterate loop through each successive content.nodes object produced by matrix loop
for (n in 1:length(content.nodes)) {
# add successively extracted content to vector
extracted.content.v <- append(extracted.content.v, xmlValue(content.nodes[[n]]))
}
# collect output of nested loop in vector of all sWords for each token in chunk
combined.content <- append(combined.content, extracted.content.v)
}
# change sWord.contents vector to lower case
combined.content <- tolower(combined.content)
# create a contingency table of sWord.contents. The table lists nuber of occurences for all sWords.
sWord.table <- table(combined.content)
# normalize sWord.table by dividing by total sWords. Multiply by 100 to produce rate of sWord occurence per 100 sWords.
sWord.freq.table <- 100*(sWord.table/sum(sWord.table))
# insert sWord.freq.table into list
sWord.freq.table.list[[length(sWord.freq.table.list)+1]] <- sWord.freq.table
}
k <- 1
}
rm(list = ls())
source("code/corpusFunctions.R")
input.dir <- "./working_input3"
files.v <- dir(path=input.dir, pattern=".*xml")
i <- 1
# create list object with no content. Vectors extracted from XML files will be stored here.
sWord.freq.table.list <- list()
# set up loop to process each file in source directory
for (i in 1:length(files.v)) {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# extract all <word> elements and children into XmlNodeList object
word.nodes <- getNodeSet(doc.object, "//word")
# here we must split files into chunks
divisor <- length(word.nodes)/98
max.length <- length(word.nodes)/divisor
x <- seq_along(word.nodes)
chunks.l <- split(word.nodes, ceiling(x/max.length))
# set up a loop to process each chunk produced by preceding code in matrix
# set up list to store results of processsing
sWord.nodes.l <- list()
# extract sWord elements from each chunk
for (k in 1:length(chunks.l)) {
sWord.nodes.l[[k]] <- sapply(chunks.l[[k]], xmlChildren)
# create vector object to hold results
combined.content <- NULL
# set increment counter to 1
m <- 1
for (m in 1:length(sWord.nodes.l[[k]])) {
# extract sWord nodes for 1 token at a tiime; result is an XML list object with content which
# includes sWord xml tags.
single.token <- sWord.nodes.l[[k]][m]
# strip sWord level xml tags from data; result is a xml list object
content.nodes <- sapply(single.token[[1]], xmlChildren)
# create vector for output
extracted.content.v <- NULL
# set increment counter to 1
n <- 1
# iterate loop through each successive content.nodes object produced by matrix loop
for (n in 1:length(content.nodes)) {
# add successively extracted content to vector
extracted.content.v <- append(extracted.content.v, xmlValue(content.nodes[[n]]))
}
# collect output of nested loop in vector of all sWords for each token in chunk
combined.content <- append(combined.content, extracted.content.v)
}
# change sWord.contents vector to lower case
combined.content <- tolower(combined.content)
# create a contingency table of sWord.contents. The table lists nuber of occurences for all sWords.
sWord.table <- table(combined.content)
# normalize sWord.table by dividing by total sWords. Multiply by 100 to produce rate of sWord occurence per 100 sWords.
sWord.freq.table <- 100*(sWord.table/sum(sWord.table))
# insert sWord.freq.table into list
sWord.freq.table.list[[length(sWord.freq.table.list)+1]] <- sWord.freq.table
}
k <- 1
}
single.token[[1]]
xmlChildren(single.token[[1]])
sapply(single.token[[1]], xmlChildren)
single.token[[1]]
xmlChildren(single.token[[1]])
length(sWord.freq.table.list)
sum(lengths(sWord.freq.table.list))
# set up loop to process each file in source directory
for (i in 36:length(files.v)) {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# extract all <word> elements and children into XmlNodeList object
word.nodes <- getNodeSet(doc.object, "//word")
# here we must split files into chunks
divisor <- length(word.nodes)/98
max.length <- length(word.nodes)/divisor
x <- seq_along(word.nodes)
chunks.l <- split(word.nodes, ceiling(x/max.length))
# set up a loop to process each chunk produced by preceding code in matrix
# set up list to store results of processsing
sWord.nodes.l <- list()
# extract sWord elements from each chunk
for (k in 1:length(chunks.l)) {
sWord.nodes.l[[k]] <- sapply(chunks.l[[k]], xmlChildren)
# create vector object to hold results
combined.content <- NULL
# set increment counter to 1
m <- 1
for (m in 1:length(sWord.nodes.l[[k]])) {
# extract sWord nodes for 1 token at a tiime; result is an XML list object with content which
# includes sWord xml tags.
single.token <- sWord.nodes.l[[k]][m]
# strip sWord level xml tags from data; result is a xml list object
content.nodes <- sapply(single.token[[1]], xmlChildren)
# create vector for output
extracted.content.v <- NULL
# set increment counter to 1
n <- 1
# iterate loop through each successive content.nodes object produced by matrix loop
for (n in 1:length(content.nodes)) {
# add successively extracted content to vector
extracted.content.v <- append(extracted.content.v, xmlValue(content.nodes[[n]]))
}
# collect output of nested loop in vector of all sWords for each token in chunk
combined.content <- append(combined.content, extracted.content.v)
}
# change sWord.contents vector to lower case
combined.content <- tolower(combined.content)
# create a contingency table of sWord.contents. The table lists nuber of occurences for all sWords.
sWord.table <- table(combined.content)
# normalize sWord.table by dividing by total sWords. Multiply by 100 to produce rate of sWord occurence per 100 sWords.
sWord.freq.table <- 100*(sWord.table/sum(sWord.table))
# insert sWord.freq.table into list
sWord.freq.table.list[[length(sWord.freq.table.list)+1]] <- sWord.freq.table
}
k <- 1
}
# !! stop here for evaluation
length(sWord.freq.table.list)
sum(lengths(sWord.freq.table.list))
summary(lengths(sWord.freq.table.list))
sWord.freq.table.list[[10]][1]
i <- 1
freqs.l <- list()
for (i in 1:length(sWord.freq.table.list)) {
freqs.l[[i]] <-data.frame(sWord.freq.table.list[[i]], ID=seq_along(sWord.freq.table.list[[i]]),
stringsAsFactors=FALSE )
}
freqs.df <- do.call(rbind, freqs.l)
dim(freqs.df)
bookids.v <- gsub(".xml", "", files.v)
chunk.total <- NULL
i <- 1
for (i in 1:length(files.v))  {
# read xml structure from file to .R object
doc.object <- xmlTreeParse(file.path(input.dir, files.v[i]), useInternalNodes=TRUE)
# extract all <word> elements and children into XmlNodeList object
word.nodes <- getNodeSet(doc.object, "//word")
# here we must split files into chunks
divisor <- length(word.nodes)/98
max.length <- length(word.nodes)/divisor
x <- seq_along(word.nodes)
chunks.l <- split(word.nodes, ceiling(x/max.length))
chunk.total <- append(chunk.total, length(chunks.l))
}
i <- 1
chunk.number <- NULL
ID.holder <- NULL
n <- 1
for (i in 1:length(chunk.total)) {
chunk.number <- seq_along(1:chunk.total[i])
# chunk.number corresponds to the different file names to used in row IDs
for (j in 1:length(chunk.number)) {
single.name <- paste(bookids.v[i], chunk.number[j], sep = "-", collapse = NULL)
ID.holder <- append(ID.holder, rep(single.name, nrow(freqs.l[[n]])))
n <-n+1
}
j <- 1
}
ID.holder[100000]
freqs.df$ID <- ID.holder
result.t <- xtabs(Freq ~ ID+combined.content, data=freqs.df)
